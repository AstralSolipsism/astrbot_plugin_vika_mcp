import json
import traceback
import os
import time
import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timedelta

from astrbot.core.platform.astr_message_event import AstrMessageEvent
from astrbot.api.event import filter
from astrbot.api.star import Context, Star, register
from astrbot.api import logger, AstrBotConfig


try:
    from astral_vika import Vika
except ImportError:
    logger.error("ç»´æ ¼è¡¨MCPæ’ä»¶å¯åŠ¨å¤±è´¥ï¼šæœªæ‰¾åˆ° astral_vika åº“ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–å®‰è£…")
    Vika = None


@register("vika_mcp_plugin", "AstralSolipsism", "æ™ºèƒ½ç»´æ ¼è¡¨MCPæ’ä»¶ï¼Œæ”¯æŒè‡ªåŠ¨å‘ç°æ•°æ®è¡¨ï¼Œå¤§æ¨¡å‹è‡ªåŠ¨è°ƒç”¨å„ç§æ“ä½œåŠŸèƒ½", "0.9")
class VikaMcpPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config
        self.vika_client = None
        self.plugin_config = {}
        self.sync_lock = asyncio.Lock()
        
        # æ™ºèƒ½æ•°æ®è¡¨ç®¡ç†
        self.datasheet_mapping = {}  # è‡ªåŠ¨å‘ç°çš„æ•°æ®è¡¨æ˜ å°„ {name: id}
        self.spaces_list = []  # ç©ºé—´ç«™åˆ—è¡¨
        self.default_space_id = None
        self.cache_file_path = os.path.join(os.getcwd(), "data", "vika_cache.json")
        self.cache_timestamp = None
        self.is_synced = False
        
        # åŠ è½½æ’ä»¶é…ç½®
        self._load_config()
        
        # åˆå§‹åŒ–ç»´æ ¼è¡¨å®¢æˆ·ç«¯
        self._init_vika_client()
        
        # åŠ è½½ç¼“å­˜
        self._load_cache()

    def _load_config(self):
        """åŠ è½½æ’ä»¶é…ç½®"""
        try:
            plugin_name = "vika_mcp_plugin"
            self.plugin_config = self.config or {}
            self.default_space_id = self.plugin_config.get('default_space_id', '')
            logger.info(f"ç»´æ ¼è¡¨MCPæ’ä»¶é…ç½®åŠ è½½å®Œæˆ: {list(self.plugin_config.keys())}")
        except Exception as e:
            logger.error(f"åŠ è½½ç»´æ ¼è¡¨MCPæ’ä»¶é…ç½®å¤±è´¥: {e}")
            self.plugin_config = {}

    def _init_vika_client(self):
        """åˆå§‹åŒ–ç»´æ ¼è¡¨å®¢æˆ·ç«¯"""
        if not Vika:
            logger.error("ç»´æ ¼è¡¨MCPæ’ä»¶åˆå§‹åŒ–å¤±è´¥ï¼švika åº“æœªå®‰è£…")
            return
            
        api_token = self.plugin_config.get('vika_api_token', '')
        vika_host = self.plugin_config.get('vika_host', 'https://api.vika.cn')
        
        if not api_token:
            logger.warning("ç»´æ ¼è¡¨ API Token æœªé…ç½®")
            return
            
        try:
            # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œæ”¯æŒè‡ªå®šä¹‰host
            self.vika_client = Vika(api_token, api_base=vika_host)
            logger.info(f"ç»´æ ¼è¡¨å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒæœåŠ¡å™¨: {vika_host}")
        except Exception as e:
            logger.error(f"ç»´æ ¼è¡¨å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

    def _load_cache(self):
        """åŠ è½½ç¼“å­˜çš„æ•°æ®è¡¨ä¿¡æ¯"""
        try:
            if os.path.exists(self.cache_file_path):
                with open(self.cache_file_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    
                self.datasheet_mapping = cache_data.get('datasheet_mapping', {})
                self.spaces_list = cache_data.get('spaces_list', [])
                self.cache_timestamp = cache_data.get('timestamp', 0)
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                cache_duration = self.plugin_config.get('cache_duration_hours', 24)
                if time.time() - self.cache_timestamp < cache_duration * 3600:
                    self.is_synced = True
                    logger.info(f"ä»ç¼“å­˜åŠ è½½äº† {len(self.datasheet_mapping)} ä¸ªæ•°æ®è¡¨")
                else:
                    logger.info("ç¼“å­˜å·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°åŒæ­¥")
            else:
                logger.info("æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œéœ€è¦åˆå§‹åŒæ­¥")
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(self.cache_file_path), exist_ok=True)
            cache_data = {
                'datasheet_mapping': self.datasheet_mapping,
                'spaces_list': self.spaces_list,
                'timestamp': time.time()
            }
            with open(self.cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logger.info("ç¼“å­˜å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _get_datasheet_id(self, datasheet_name: str) -> str:
        """æ™ºèƒ½è·å–æ•°æ®è¡¨ID"""
        # 1. é¦–å…ˆæ£€æŸ¥è‡ªåŠ¨å‘ç°çš„æ˜ å°„
        if datasheet_name in self.datasheet_mapping:
            datasheet_id = self.datasheet_mapping[datasheet_name]
            return datasheet_id
            
        # 2. æ£€æŸ¥è‡ªå®šä¹‰åˆ«å
        custom_aliases = self.plugin_config.get('custom_aliases', {})
        if datasheet_name in custom_aliases:
            real_name = custom_aliases[datasheet_name]
            if real_name in self.datasheet_mapping:
                datasheet_id = self.datasheet_mapping[real_name]
                return datasheet_id
                
        # 3. æ£€æŸ¥æ‰‹åŠ¨é…ç½®çš„æ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
        manual_mapping = self.plugin_config.get('datasheet_mapping', {})
        if datasheet_name in manual_mapping:
            datasheet_id = manual_mapping[datasheet_name]
            return datasheet_id
            
        # 4. å¦‚æœæ˜¯ç›´æ¥çš„datasheet IDæ ¼å¼ï¼Œç›´æ¥è¿”å›
        if datasheet_name.startswith('dst'):
            return datasheet_name
            
        # 5. æ¨¡ç³ŠåŒ¹é…ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
        for name, ds_id in self.datasheet_mapping.items():
            if datasheet_name.lower() in name.lower() or name.lower() in datasheet_name.lower():
                return ds_id
                
        # 6. éƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸåç§°
        logger.warning(f"æœªèƒ½é€šè¿‡ä»»ä½•å·²çŸ¥æ–¹å¼è§£ææ•°æ®è¡¨åç§° '{datasheet_name}'ã€‚å°†æŒ‰åŸæ ·ä½¿ç”¨ã€‚")
        return datasheet_name

    def _get_datasheet(self, datasheet_name: str):
        """è·å–æ•°æ®è¡¨å¯¹è±¡"""
        if not self.vika_client:
            raise ValueError("ç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokenå’ŒæœåŠ¡å™¨é…ç½®")
        
        datasheet_id = self._get_datasheet_id(datasheet_name)
        if not datasheet_id:
            raise ValueError(f"æœªæ‰¾åˆ°æ•°æ®è¡¨: {datasheet_name}")
        
        return self.vika_client.datasheet(datasheet_id)
        
    async def _traverse_node_recursive(self, node_id: str, datasheet_map: Dict[str, str], depth: int = 0, space_id: str = None):
        """é€’å½’éå†èŠ‚ç‚¹ï¼Œå‘ç°æ‰€æœ‰æ•°æ®è¡¨"""
        if depth > 10:  # é˜²æ­¢æ— é™é€’å½’
            logger.warning(f"é€’å½’æ·±åº¦è¶…è¿‡é™åˆ¶ï¼Œåœæ­¢éå†èŠ‚ç‚¹: {node_id}")
            return
            
        try:
            node_detail = await self.vika_client.space(space_id).nodes.aget(node_id)
            
            if hasattr(node_detail, 'type'):
                if node_detail.type == 'Datasheet':
                    datasheet_map[node_detail.name] = node_detail.id
                    logger.info(f"å‘ç°æ•°æ®è¡¨: {node_detail.name} ({node_detail.id})")
                elif node_detail.type == 'Folder':
                    # å¯¹äºæ–‡ä»¶å¤¹èŠ‚ç‚¹ï¼Œéœ€è¦æ˜¾å¼åœ°é€šè¿‡APIè·å–å…¶å­èŠ‚ç‚¹
                    children_nodes = node_detail.children
                    
                    child_datasheets = [child.name for child in children_nodes if child.type == 'Datasheet']
                    if child_datasheets:
                        logger.info(f"åœ¨æ–‡ä»¶å¤¹ [{node_detail.name}] ä¸‹å‘ç°ç»´æ ¼è¡¨: {child_datasheets}")
        
                    for child in children_nodes:
                        await self._traverse_node_recursive(child.id, datasheet_map, depth + 1, space_id) # ä¼ é€’ space_id
        except Exception as e:
            logger.error(f"éå†èŠ‚ç‚¹ {node_id} æ—¶å‡ºé”™: {e}")

    async def _auto_sync_if_needed(self):
        """å¦‚æœéœ€è¦ä¸”é…ç½®å…è®¸ï¼Œè‡ªåŠ¨åŒæ­¥æ•°æ®è¡¨"""
        if (not self.is_synced and
            self.plugin_config.get('auto_sync_on_startup', True) and
            self.vika_client):
            async with self.sync_lock:
                if self.is_synced:  # åœ¨é”å†…è¿›è¡ŒåŒé‡æ£€æŸ¥
                    return
                # ç°åœ¨è®©è°ƒç”¨è€…å¤„ç†å¼‚å¸¸
                await self._perform_sync()

    async def _perform_sync(self):
        """æ‰§è¡Œæ•°æ®è¡¨åŒæ­¥"""
        if not self.vika_client:
            raise ValueError("ç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
        # è·å–ç©ºé—´ç«™åˆ—è¡¨
        spaces = await self.vika_client.spaces.alist()
        
        if not spaces:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•ç©ºé—´ç«™ï¼Œè¯·æ£€æŸ¥API Tokenæƒé™")
            
        self.spaces_list = [{'id': space['id'], 'name': space['name']} for space in spaces]
        
        # ç¡®å®šè¦åŒæ­¥çš„ç©ºé—´ç«™
        target_space = None
        if self.default_space_id:
            target_space = next((s for s in spaces if s['id'] == self.default_space_id), None)
        
        if not target_space:
            target_space = spaces[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç©ºé—´ç«™
            
        logger.info(f"å¼€å§‹åŒæ­¥ç©ºé—´ç«™: {target_space['name']} (ID: {target_space['id']})")
        
        # ä½¿ç”¨ v2 API ç›´æ¥æœç´¢æ‰€æœ‰æ•°æ®è¡¨èŠ‚ç‚¹ï¼Œä¼˜åŒ–åŒæ­¥æ•ˆç‡
        logger.info(f"é«˜æ•ˆåŒæ­¥ç©ºé—´ç«™ [{target_space['name']}] ä¸­çš„æ‰€æœ‰æ•°æ®è¡¨...")
        all_datasheet_nodes = await self.vika_client.space(target_space['id']).nodes.asearch(type='Datasheet')
        
        new_datasheet_map = {node.name: node.id for node in all_datasheet_nodes}
            
        logger.info(f"åŒæ­¥å®Œæˆã€‚å‘ç°çš„ç»´æ ¼è¡¨æ˜ å°„: {new_datasheet_map}")
        self.datasheet_mapping.update(new_datasheet_map)
        
        # è®°å½•åŒæ­¥ä¿¡æ¯
        if new_datasheet_map:
            synced_table_names = list(new_datasheet_map.keys())
            logger.info(f"æˆåŠŸåŒæ­¥ {len(synced_table_names)} ä¸ªç»´æ ¼è¡¨: {synced_table_names}")
            
        self.is_synced = True
        self.cache_timestamp = time.time()
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_cache()
        
        return len(new_datasheet_map)

    def _format_records_to_json(self, records: List[Any]) -> str:
        """å°†è®°å½•åˆ—è¡¨æ ¼å¼åŒ–ä¸ºJSONå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿AIç†è§£ã€‚"""
        if not records:
            return json.dumps([])

        record_list = []
        for record in records:
            # å‡è®¾ record å¯¹è±¡æœ‰ .id å’Œ .fields å±æ€§
            record_dict = {
                "record_id": getattr(record, 'id', ''),
                "fields": getattr(record, 'fields', {})
            }
            record_list.append(record_dict)
        
        return json.dumps(record_list, ensure_ascii=False, indent=2)

    def _format_records_for_display(self, records: List[Dict[str, Any]], limit: int = None) -> str:
        """æ ¼å¼åŒ–è®°å½•ä¸ºå¯è¯»çš„æ–‡æœ¬æ ¼å¼"""
        if not records:
            return "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®°å½•"

        max_display = limit or self.plugin_config.get('max_records_display', 20)
        display_records = records[:max_display]
        
        # æ„å»ºè¡¨æ ¼æ˜¾ç¤º
        result = f"æ‰¾åˆ° {len(records)} æ¡è®°å½•"
        if len(records) > max_display:
            result += f"ï¼ˆæ˜¾ç¤ºå‰ {max_display} æ¡ï¼‰"
        result += ":\\n\\n"
        
        if display_records:
            # è·å–æ‰€æœ‰å­—æ®µå
            all_fields = set()
            for record in display_records:
                record_fields = getattr(record, 'fields', None)
                if record_fields:
                    all_fields.update(record_fields.keys())
            
            field_list = list(all_fields)
            
            # è¡¨å¤´
            result += "| " + " | ".join(field_list) + " |\\n"
            result += "|" + "---|" * len(field_list) + "\\n"
            
            # æ•°æ®è¡Œ
            for record in display_records:
                fields = getattr(record, 'fields', {})
                row_data = []
                for field in field_list:
                    value = fields.get(field, '')
                    # å¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œé•¿æ–‡æœ¬
                    if isinstance(value, str):
                        value = value.replace('|', '\\|').replace('\\n', ' ')
                        if len(value) > 30:
                            value = value[:30] + "..."
                    elif value is None:
                        value = ""
                    row_data.append(str(value))
                result += "| " + " | ".join(row_data) + " |\\n"
            
        return result

    def _parse_field_data(self, field_data_str: str) -> Dict[str, Any]:
        """è§£æå­—æ®µæ•°æ®å­—ç¬¦ä¸²ä¸ºå­—å…¸"""
        try:
            # å°è¯•è§£æJSONæ ¼å¼
            data = json.loads(field_data_str)
            return data
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯JSONï¼Œå°è¯•è§£æé”®å€¼å¯¹æ ¼å¼
            fields = {}
            pairs = field_data_str.split(',')
            for pair in pairs:
                if '=' in pair:
                    key, value = pair.split('=', 1)
                    key = key.strip().strip('"').strip("'")
                    value = value.strip().strip('"').strip("'")
                    
                    # è‡ªåŠ¨ç±»å‹è½¬æ¢
                    if self.plugin_config.get('enable_auto_type_conversion', True):
                        if value.isdigit():
                            value = int(value)
                        elif value.replace('.', '', 1).isdigit():
                            value = float(value)
                        elif value.lower() in ['true', 'false']:
                            value = value.lower() == 'true'
                    
                    fields[key] = value
            return fields

    @filter.llm_tool(name="sync_vika_datasheets")
    async def sync_vika_datasheets(self, event: AstrMessageEvent) -> str:
        """åŒæ­¥å¹¶ç¼“å­˜ç»´æ ¼ç©ºé—´ç«™ä¸­çš„æ‰€æœ‰æ•°æ®è¡¨ï¼Œè®©æ‚¨å¯ä»¥é€šè¿‡åç§°ç›´æ¥æ“ä½œå®ƒä»¬ã€‚
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        try:
            discovered_count = await self._perform_sync()
            
            result = f"âœ… åŒæ­¥å®Œæˆï¼å…±å‘ç° {discovered_count} ä¸ªæ•°æ®è¡¨ã€‚\n\n"
            result += "ğŸ“‹ **å·²å‘ç°çš„æ•°æ®è¡¨**ï¼š\n"
            
            for name, ds_id in list(self.datasheet_mapping.items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                result += f"â€¢ {name} (`{ds_id}`)\n"
                
            if len(self.datasheet_mapping) > 10:
                result += f"â€¢ ... è¿˜æœ‰ {len(self.datasheet_mapping) - 10} ä¸ªæ•°æ®è¡¨\n"
                
            result += "\nğŸ’¡ ç°åœ¨æ‚¨å¯ä»¥ç›´æ¥é€šè¿‡æ•°æ®è¡¨åç§°æ¥æ“ä½œå®ƒä»¬äº†ï¼"
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ åŒæ­¥ç»´æ ¼è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="list_vika_spaces")
    async def list_vika_spaces(self, event: AstrMessageEvent) -> str:
        """åˆ—å‡ºæ‚¨åœ¨ç»´æ ¼è¡¨å¹³å°ä¸­åˆ›å»ºæˆ–æœ‰æƒè®¿é—®çš„æ‰€æœ‰ç©ºé—´ç«™ï¼ˆå³è¡¨æ ¼çš„ç»„ç»‡å®¹å™¨ï¼‰ã€‚
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        try:
            spaces = await self.vika_client.spaces.alist()
            
            if not spaces:
                return "ğŸ“­ æœªæ‰¾åˆ°ä»»ä½•ç©ºé—´ç«™ï¼Œè¯·æ£€æŸ¥æ‚¨çš„API Tokenæƒé™"
                
            result = f"ğŸ¢ **æ‚¨çš„ç»´æ ¼è¡¨ç©ºé—´ç«™** (å…± {len(spaces)} ä¸ª)ï¼š\n\n"
            
            for space in spaces:
                is_default = " ğŸ”¸ *é»˜è®¤*" if space['id'] == self.default_space_id else ""
                result += f"â€¢ **{space['name']}**{is_default}\n"
                result += f"  ID: `{space['id']}`\n\n"
                
            if not self.default_space_id and len(spaces) > 1:
                result += "ğŸ’¡ **æç¤º**: å¦‚æœæ‚¨æœ‰å¤šä¸ªç©ºé—´ç«™ï¼Œå»ºè®®åœ¨é…ç½®ä¸­è®¾ç½® `default_space_id` ä»¥æŒ‡å®šé»˜è®¤æ“ä½œçš„ç©ºé—´ç«™ã€‚"
                
            return result
            
        except Exception as e:
            error_msg = f"âŒ è·å–ç©ºé—´ç«™åˆ—è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="list_vika_datasheets")
    async def list_vika_datasheets(self, event: AstrMessageEvent, space_id: str = None, filter_keyword: str = None, recursive_search: bool = False) -> str:
        """åˆ—å‡ºç»´æ ¼è¡¨ä¸­çš„æ‰€æœ‰å…·ä½“æ•°æ®è¡¨æ ¼ï¼ˆDatasheetï¼‰ï¼Œæ”¯æŒæŒ‡å®šç©ºé—´ç«™å’Œå…³é”®è¯è¿‡æ»¤ã€‚

        Args:
            space_id(string): å¯é€‰ï¼ŒæŒ‡å®šè¦åˆ—å‡ºæ•°æ®è¡¨çš„ç©ºé—´ç«™IDã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨é»˜è®¤ç©ºé—´ç«™æˆ–å·²åŒæ­¥çš„æ•°æ®è¡¨ã€‚
            filter_keyword(string): å¯é€‰ï¼Œè¿‡æ»¤å…³é”®è¯ï¼Œåªæ˜¾ç¤ºåŒ…å«è¯¥å…³é”®è¯çš„æ•°æ®è¡¨ã€‚
            recursive_search(boolean): å¯é€‰ï¼Œé»˜è®¤ä¸ºFalseã€‚å¦‚æœä¸ºTrueï¼Œå°†ä½¿ç”¨é€’å½’æ–¹å¼éå†æ–‡ä»¶å¤¹æ¥æŸ¥æ‰¾æ•°æ®è¡¨ï¼Œå¯ä»¥å‘ç°æ–‡ä»¶å¤¹ä¿¡æ¯ä½†é€Ÿåº¦è¾ƒæ…¢ï¼›å¦‚æœä¸ºFalseï¼Œå°†ä½¿ç”¨é«˜æ•ˆçš„æœç´¢APIï¼Œé€Ÿåº¦å¿«ä½†æ— æ³•å±•ç¤ºæ–‡ä»¶å¤¹å±‚çº§ã€‚
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"

        try:
            datasheets_to_list = {}
            space_id_to_query = space_id
            
            # å¦‚æœç”¨æˆ·æ²¡æœ‰æä¾› space_idï¼Œåˆ™æ£€æŸ¥å¹¶ä½¿ç”¨é»˜è®¤é…ç½®
            if not space_id_to_query:
                space_id_to_query = self.default_space_id

            if space_id_to_query and isinstance(space_id_to_query, str):
                try:
                    if recursive_search:
                        # å¦‚æœå¯ç”¨äº†é€’å½’æœç´¢ï¼Œåˆ™ä½¿ç”¨æ—§çš„éå†æ–¹æ³•
                        logger.info(f"ä½¿ç”¨é€’å½’æ–¹å¼éå†ç©ºé—´ç«™ [{space_id_to_query}]...")
                        root_nodes = await self.vika_client.space(space_id_to_query).nodes.alist()
                        temp_map = {}
                        for node in root_nodes:
                            await self._traverse_node_recursive(node.id, temp_map, space_id=space_id_to_query)
                        datasheets_to_list = temp_map
                    else:
                        # é»˜è®¤ä½¿ç”¨é«˜æ•ˆçš„æœç´¢API
                        logger.info(f"ä½¿ç”¨é«˜æ•ˆæœç´¢APIåœ¨ç©ºé—´ç«™ [{space_id_to_query}] ä¸­æŸ¥æ‰¾æ‰€æœ‰æ•°æ®è¡¨...")
                        all_datasheet_nodes = await self.vika_client.space(space_id_to_query).nodes.asearch(type='Datasheet')
                        datasheets_to_list = {node.name: node.id for node in all_datasheet_nodes}
                except Exception as e:
                    logger.error(f"æ— æ³•è·å–ç©ºé—´ç«™ '{space_id_to_query}' ä¸­çš„æ•°æ®è¡¨: {e}\n{traceback.format_exc()}")
                    return f"âŒ æ— æ³•è·å–ç©ºé—´ç«™ '{space_id_to_query}' ä¸­çš„æ•°æ®è¡¨ï¼Œè¯·æ£€æŸ¥ç©ºé—´ç«™IDå’Œæƒé™: {str(e)}"
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šspace_idï¼Œåˆ™ä½¿ç”¨å·²åŒæ­¥çš„æ•°æ®è¡¨
                await self._auto_sync_if_needed()
                if not self.is_synced:
                    return "âš ï¸ æ•°æ®è¡¨åˆ—è¡¨å°šæœªåŒæ­¥ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®è¡¨åŒæ­¥åŠŸèƒ½ã€‚"
                datasheets_to_list = self.datasheet_mapping

            if not datasheets_to_list:
                return "ğŸ“­ æœªå‘ç°ä»»ä½•æ•°æ®è¡¨ï¼Œè¯·æ£€æŸ¥ç©ºé—´ç«™ä¸­æ˜¯å¦æœ‰æ•°æ®è¡¨ï¼Œæˆ–é‡æ–°åŒæ­¥ã€‚"

            # åº”ç”¨è¿‡æ»¤
            filtered_tables = {}
            if filter_keyword:
                for name, ds_id in datasheets_to_list.items():
                    if filter_keyword.lower() in name.lower():
                        filtered_tables[name] = ds_id
            else:
                filtered_tables = datasheets_to_list

            if not filtered_tables:
                return f"ğŸ” æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ '{filter_keyword}' çš„æ•°æ®è¡¨ã€‚"

            result = f"ğŸ“Š **æ•°æ®è¡¨åˆ—è¡¨**"
            if space_id_to_query:
                result += f" (æ¥è‡ªç©ºé—´ç«™: `{space_id_to_query}`)"
            if filter_keyword:
                result += f" (åŒ…å« '{filter_keyword}')"
            result += f" (å…± {len(filtered_tables)} ä¸ª)ï¼š\n\n"

            for name, ds_id in filtered_tables.items():
                result += f"â€¢ **{name}**\n"
                result += f"  ID: `{ds_id}`\n\n"

            # æ˜¾ç¤ºè‡ªå®šä¹‰åˆ«åæç¤º
            custom_aliases = self.plugin_config.get('custom_aliases', {})
            if custom_aliases:
                result += "ğŸ·ï¸ **è‡ªå®šä¹‰åˆ«å**ï¼š\n"
                for alias, real_name in custom_aliases.items():
                    if real_name in self.datasheet_mapping: # ä»…æ˜¾ç¤ºå·²åŒæ­¥çš„åˆ«å
                        result += f"â€¢ `{alias}` â†’ {real_name}\n"
                result += "\n"

            result += "ğŸ’¡ **æç¤º**: æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ•°æ®è¡¨åç§°è¿›è¡Œæ“ä½œï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«ã€‚"
            
            return result

        except Exception as e:
            error_msg = f"âŒ æŸ¥è¯¢è¡¨æ ¼å¤±è´¥ï¼š{str(e)}ã€‚è¯·æ£€æŸ¥æ‚¨çš„API Tokenæƒé™æˆ–ç©ºé—´ç«™IDæ˜¯å¦æ­£ç¡®ï¼Œå¹¶æŸ¥çœ‹åå°æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚"
            logger.error(f"è·å–æ•°æ®è¡¨åˆ—è¡¨å¤±è´¥: {e}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="create_vika_datasheet")
    async def create_vika_datasheet(self, event: AstrMessageEvent, datasheet_name: str, fields: list) -> str:
        """åœ¨ç»´æ ¼è¡¨ç©ºé—´ç«™ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®è¡¨ã€‚

        Args:
            datasheet_name(string): è¦åˆ›å»ºçš„æ•°æ®è¡¨çš„åç§°ã€‚
            fields(array): å­—æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå­—æ®µæ˜¯ä¸€ä¸ªåŒ…å« "name" å’Œ "type" çš„å­—å…¸ã€‚
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        try:
            # ç¡®å®šç›®æ ‡ç©ºé—´ç«™
            spaces = await self.vika_client.spaces.alist()
            
            if not spaces:
                return "âŒ æœªæ‰¾åˆ°ä»»ä½•ç©ºé—´ç«™ï¼Œè¯·æ£€æŸ¥API Tokenæƒé™"
                
            target_space = None
            if self.default_space_id:
                target_space = next((s for s in spaces if s['id'] == self.default_space_id), None)
            
            if not target_space:
                target_space = spaces[0]
                
            # åˆ›å»ºæ•°æ®è¡¨
            create_params = {
                'name': datasheet_name,
                'fields': fields,
                'folderId': target_space['id']
            }
            
            new_datasheet = await self.vika_client.space(target_space['id']).datasheets.acreate(**create_params)
            
            # æ›´æ–°æœ¬åœ°ç¼“å­˜
            self.datasheet_mapping[datasheet_name] = new_datasheet.id
            self._save_cache()
            
            result = f"âœ… æ•°æ®è¡¨åˆ›å»ºæˆåŠŸï¼\n\n"
            result += f"ğŸ“Š **æ•°æ®è¡¨åç§°**: {datasheet_name}\n"
            result += f"ğŸ†” **æ•°æ®è¡¨ID**: `{new_datasheet.id}`\n"
            result += f"ğŸ¢ **æ‰€åœ¨ç©ºé—´ç«™**: {target_space['name']}\n"
            
            if fields:
                result += f"ğŸ“‹ **å­—æ®µæ•°é‡**: {len(fields)} ä¸ª\n"
                
            result += "\nğŸ’¡ æ•°æ®è¡¨å·²æ·»åŠ åˆ°ç¼“å­˜ï¼Œæ‚¨å¯ä»¥ç›´æ¥é€šè¿‡åç§°æ“ä½œå®ƒã€‚"
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ åˆ›å»ºæ•°æ®è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    async def _check_sync_and_suggest(self, datasheet_name: str) -> Optional[str]:
        """æ£€æŸ¥æ•°æ®è¡¨æ˜¯å¦åŒæ­¥ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™æä¾›å»ºè®®"""
        if not self.vika_client:
            return "âŒ ç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"

        # ç¡®ä¿åœ¨æ£€æŸ¥å‰å·²å°è¯•åŒæ­¥
        await self._auto_sync_if_needed()
            
        # å°è¯•æ™ºèƒ½è·å–æ•°æ®è¡¨ID
        datasheet_id = self._get_datasheet_id(datasheet_name)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®è¡¨ï¼Œæä¾›å»ºè®®
        if datasheet_id == datasheet_name and not datasheet_name.startswith('dst'):
            if not self.is_synced:
                msg = f"âš ï¸ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}'ï¼Œä¸”æ•°æ®è¡¨åˆ—è¡¨è‡ªåŠ¨åŒæ­¥å¤±è´¥ã€‚è¯·æ£€æŸ¥é…ç½®æˆ–æ‰‹åŠ¨åŒæ­¥ã€‚"
                return msg
            else:
                # æä¾›ç›¸ä¼¼çš„æ•°æ®è¡¨å»ºè®®
                similar_tables = []
                for table_name in self.datasheet_mapping.keys():
                    if any(word in table_name.lower() for word in datasheet_name.lower().split()):
                        similar_tables.append(table_name)
                
                if similar_tables:
                    suggestion = f"âŒ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}'ã€‚æ‚¨æ˜¯å¦æƒ³è¦æ“ä½œï¼š\n"
                    for table in similar_tables[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªå»ºè®®
                        suggestion += f"â€¢ {table}\n"
                    return suggestion
                else:
                    available_tables = list(self.datasheet_mapping.keys())[:5]
                    msg = (f"âŒ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}'ã€‚\n"
                           f"å¯ç”¨çš„æ•°æ®è¡¨ï¼š{', '.join(available_tables)}")
                    return msg
        
        return None  # æ‰¾åˆ°äº†æ•°æ®è¡¨ï¼Œæ— éœ€æç¤º

    @filter.llm_tool(name="query_vika_datasheet")
    async def query_vika_datasheet(self, event: AstrMessageEvent, datasheet_name: str, formula: str = None):
        """
        æŸ¥è¯¢å¹¶è¿”å›æŒ‡å®šç»´æ ¼è¡¨ä¸­çš„å†…å®¹ã€‚æ”¯æŒä½¿ç”¨å…¬å¼è¿›è¡Œç²¾ç¡®è¿‡æ»¤ã€‚

        Args:
            datasheet_name(string): è¦æŸ¥è¯¢çš„æ•°æ®è¡¨çš„å‡†ç¡®åç§°ã€‚
            formula(string): å¯é€‰ï¼Œä¸€ä¸ªç»´æ ¼è¡¨æŸ¥è¯¢å…¬å¼ï¼Œç”¨äºè¿‡æ»¤è®°å½•ã€‚ä¾‹å¦‚ï¼š"AND({çŠ¶æ€}='å·²å®Œæˆ', {è´Ÿè´£äºº}='å¼ ä¸‰')"
        """
        try:
            if not self.vika_client:
                return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"

            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
            
            datasheet = self._get_datasheet(datasheet_name)
            logger.info(f"æˆåŠŸå®šä½æ•°æ®è¡¨: {datasheet.dst_id}")

            # æ ¹æ®æ˜¯å¦æœ‰å…¬å¼æ¥å†³å®šæŸ¥è¯¢æ–¹å¼
            if formula:
                logger.info(f"ä½¿ç”¨å…¬å¼è¿›è¡ŒæŸ¥è¯¢: {formula}")
                # ä½¿ç”¨å…¬å¼è¿‡æ»¤è®°å½•
                records = await datasheet.records.filter(formula=formula).aall()
            else:
                logger.info("æŸ¥è¯¢æ‰€æœ‰è®°å½•")
                # è·å–æ‰€æœ‰è®°å½•
                records = await datasheet.records.aall()

            # æ ¼å¼åŒ–ä¸ºJSONå¹¶è¿”å›
            formatted_content = self._format_records_to_json(records)
            
            logger.info(f"æˆåŠŸè¯»å–æ•°æ®è¡¨ '{datasheet_name}' çš„ {len(records)} æ¡è®°å½•ã€‚")
            
            return formatted_content

        except Exception as e:
            error_msg = f"âŒ æŸ¥è¯¢ç»´æ ¼è¡¨ '{datasheet_name}' å†…å®¹å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="add_vika_record")
    async def add_vika_record(
        self,
        datasheet_name: str,
        record_data: str
    ) -> str:
        """å‘æŒ‡å®šçš„ç»´æ ¼è¡¨ä¸­æ·»åŠ æ–°è®°å½•ã€‚

        Args:
            datasheet_name(string): è¦æ·»åŠ è®°å½•çš„æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
            record_data(string): è®°å½•æ•°æ®ï¼Œå¯ä»¥æ˜¯JSONæ ¼å¼æˆ–é”®å€¼å¯¹æ ¼å¼ï¼ˆå¦‚ï¼š"å§“å=å¼ ä¸‰,å¹´é¾„=25"æˆ–'{"å§“å":"å¼ ä¸‰","å¹´é¾„":25}'ï¼‰
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # è§£æè®°å½•æ•°æ®
            fields_data = self._parse_field_data(record_data)
            
            if not fields_data:
                return "âŒ é”™è¯¯ï¼šè®°å½•æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ä½¿ç”¨JSONæ ¼å¼æˆ–é”®å€¼å¯¹æ ¼å¼ï¼ˆå¦‚ï¼š'å§“å=å¼ ä¸‰,å¹´é¾„=25'ï¼‰"
            
            # ç›´æ¥æ‰§è¡Œå¼‚æ­¥æ“ä½œ
            result = await datasheet.records.acreate(fields_data)
            
            logger.info(f"æˆåŠŸå‘ç»´æ ¼è¡¨ [{datasheet_name}] æ·»åŠ äº† 1 æ¡æ–°è®°å½•: {result.id}")
            return f"âœ… æˆåŠŸæ·»åŠ è®°å½•åˆ°æ•°æ®è¡¨ '{datasheet_name}'ï¼Œè®°å½•ID: {result.id}"
            
        except Exception as e:
            error_msg = f"âŒ æ·»åŠ ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="search_vika_records")
    async def search_vika_records(
        self,
        datasheet_name: str,
        search_query: str,
        search_fields: str = None
    ) -> str:
        """åœ¨æŒ‡å®šçš„ç»´æ ¼è¡¨ä¸­æœç´¢åŒ…å«ç‰¹å®šå†…å®¹çš„è®°å½•ã€‚

        Args:
            datasheet_name(string): è¦æœç´¢çš„æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
            search_query(string): æœç´¢å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰
            search_fields(string): å¯é€‰ï¼ŒæŒ‡å®šè¦æœç´¢çš„å­—æ®µåï¼Œå¤šä¸ªå­—æ®µç”¨é€—å·åˆ†éš”
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # è·å–æ‰€æœ‰è®°å½•
            all_records = await datasheet.records.aall()
            
            # æ‰§è¡Œæœç´¢
            matching_records = []
            query_lower = search_query.lower()
            search_field_list = None
            
            if search_fields:
                search_field_list = [f.strip() for f in search_fields.split(',')]
            
            for record in all_records:
                fields = getattr(record, 'fields', {})
                
                # ç¡®å®šè¦æœç´¢çš„å­—æ®µ
                fields_to_search = search_field_list if search_field_list else fields.keys()
                
                # åœ¨æŒ‡å®šå­—æ®µä¸­æœç´¢
                for field_name in fields_to_search:
                    if field_name in fields:
                        field_value = fields[field_name]
                        if isinstance(field_value, str) and query_lower in field_value.lower():
                            matching_records.append(record)
                            break
                        elif str(field_value).lower() == query_lower:
                            matching_records.append(record)
                            break
            
            if not matching_records:
                search_scope = f"åœ¨å­—æ®µ [{search_fields}] ä¸­" if search_fields else "åœ¨æ‰€æœ‰å­—æ®µä¸­"
                return f"ğŸ” {search_scope}æ²¡æœ‰æ‰¾åˆ°åŒ…å« '{search_query}' çš„è®°å½•"
            
            result = f"ğŸ” **æœç´¢ç»“æœ** (å…³é”®è¯: '{search_query}'):\n\n"
            result += self._format_records_for_display(matching_records)
            return result
            
        except Exception as e:
            error_msg = f"âŒ æœç´¢ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="update_vika_record")
    async def update_vika_record(self, event: AstrMessageEvent, datasheet_name: str, record_id: str, record_data: dict) -> str:
        """æ›´æ–°æŒ‡å®šç»´æ ¼è¡¨ä¸­çš„è®°å½•ã€‚

        Args:
            datasheet_name(string): è®°å½•æ‰€åœ¨çš„æ•°æ®è¡¨çš„åç§°ã€‚
            record_id(string): è¦æ›´æ–°çš„è®°å½•çš„ IDã€‚
            record_data(object): ä¸€ä¸ªåŒ…å«è¦æ›´æ–°çš„å­—æ®µå’Œæ–°å€¼çš„å­—å…¸ã€‚
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            if not record_data:
                return "âŒ é”™è¯¯ï¼šæ›´æ–°æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®"
            
            # ç›´æ¥æ‰§è¡Œå¼‚æ­¥æ“ä½œ
            # aupdate æœŸæœ›ä¸€ä¸ªåŒ…å« recordId çš„å­—å…¸æˆ– Record å¯¹è±¡
            await datasheet.records.aupdate([{'recordId': record_id, 'fields': record_data}])
            
            logger.info(f"æˆåŠŸåœ¨ç»´æ ¼è¡¨ [{datasheet_name}] ä¸­æ›´æ–°äº† 1 æ¡è®°å½•: {record_id}")
            return f"âœ… æˆåŠŸæ›´æ–°æ•°æ®è¡¨ '{datasheet_name}' ä¸­çš„è®°å½• {record_id}"
            
        except Exception as e:
            error_msg = f"âŒ æ›´æ–°ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="delete_vika_record")
    async def delete_vika_record(self, event: AstrMessageEvent, datasheet_name: str, record_id: str) -> str:
        """åˆ é™¤æŒ‡å®šç»´æ ¼è¡¨ä¸­çš„è®°å½•ã€‚

        Args:
            datasheet_name(string): è®°å½•æ‰€åœ¨çš„æ•°æ®è¡¨çš„åç§°ã€‚
            record_id(string): è¦åˆ é™¤çš„è®°å½•çš„ IDã€‚
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # ç›´æ¥æ‰§è¡Œå¼‚æ­¥æ“ä½œ
            await datasheet.records.adelete(record_id)
            
            return f"âœ… æˆåŠŸåˆ é™¤æ•°æ®è¡¨ '{datasheet_name}' ä¸­çš„è®°å½• {record_id}"
            
        except Exception as e:
            error_msg = f"âŒ åˆ é™¤ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="get_vika_fields")
    async def get_vika_fields(
        self,
        datasheet_name: str
    ) -> str:
        """è·å–æŒ‡å®šç»´æ ¼è¡¨çš„å­—æ®µä¿¡æ¯ã€‚

        Args:
            datasheet_name(string): æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # è·å–å­—æ®µä¿¡æ¯
            fields = await datasheet.fields.aall()
            
            if not fields:
                return f"ğŸ“‹ æ•°æ®è¡¨ '{datasheet_name}' æ²¡æœ‰å­—æ®µä¿¡æ¯"
            
            result = f"ğŸ“‹ **æ•°æ®è¡¨ '{datasheet_name}' çš„å­—æ®µä¿¡æ¯** (å…± {len(fields)} ä¸ªå­—æ®µ):\n\n"
            for field in fields:
                result += f"â€¢ **{field.name}** (ç±»å‹: {field.type})"
                if hasattr(field, 'description') and field.description:
                    result += f" - {field.description}"
                result += "\n"
            
            result += "\nğŸ’¡ æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›å­—æ®µåæ¥æ·»åŠ æˆ–æ›´æ–°è®°å½•ã€‚"
            return result
            
        except Exception as e:
            error_msg = f"âŒ è·å–ç»´æ ¼è¡¨å­—æ®µä¿¡æ¯å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="get_vika_status")
    async def get_vika_status(self) -> str:
        """æ£€æŸ¥ç»´æ ¼è¡¨æ’ä»¶çš„è¿æ¥çŠ¶æ€ã€é…ç½®ä¿¡æ¯å’Œæ•°æ®è¡¨åŒæ­¥çŠ¶æ€ã€‚
        """
        try:
            # ç¡®ä¿çŠ¶æ€æ£€æŸ¥å‰å·²å°è¯•åŒæ­¥
            await self._auto_sync_if_needed()
            
            status = "ğŸ”§ **ç»´æ ¼è¡¨MCPæ’ä»¶çŠ¶æ€**\n\n"
            
            # æ£€æŸ¥å®¢æˆ·ç«¯çŠ¶æ€
            if self.vika_client:
                status += "âœ… **å®¢æˆ·ç«¯è¿æ¥**: æ­£å¸¸\n"
            else:
                status += "âŒ **å®¢æˆ·ç«¯è¿æ¥**: æœªå»ºç«‹\n"
            
            # æ£€æŸ¥é…ç½®
            api_token = self.plugin_config.get('vika_api_token', '')
            vika_host = self.plugin_config.get('vika_host', 'https://api.vika.cn')
            
            if api_token:
                status += f"ğŸ”‘ **API Token**: å·²é…ç½® ({api_token[:8]}...)\n"
            else:
                status += "ğŸ”‘ **API Token**: âŒ æœªé…ç½®\n"
            
            status += f"ğŸŒ **æœåŠ¡å™¨åœ°å€**: {vika_host}\n"
            
            # ç©ºé—´ç«™ä¿¡æ¯
            if self.spaces_list:
                status += f"ğŸ¢ **ç©ºé—´ç«™**: å…± {len(self.spaces_list)} ä¸ª\n"
                if self.default_space_id:
                    default_space = next((s for s in self.spaces_list if s['id'] == self.default_space_id), None)
                    if default_space:
                        status += f"ğŸ”¸ **é»˜è®¤ç©ºé—´ç«™**: {default_space['name']}\n"
            else:
                status += "ğŸ¢ **ç©ºé—´ç«™**: æœªåŒæ­¥\n"
            
            # æ•°æ®è¡¨åŒæ­¥çŠ¶æ€
            if self.is_synced:
                cache_age = int((time.time() - self.cache_timestamp) / 3600) if self.cache_timestamp else 0
                status += f"ğŸ“Š **æ•°æ®è¡¨åŒæ­¥**: âœ… å·²åŒæ­¥ ({len(self.datasheet_mapping)} ä¸ªè¡¨)\n"
                status += f"ğŸ• **ç¼“å­˜æ—¶é—´**: {cache_age} å°æ—¶å‰\n"
                
                # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®è¡¨
                if self.datasheet_mapping:
                    status += "ğŸ“‹ **éƒ¨åˆ†æ•°æ®è¡¨**:\n"
                    for i, (name, _) in enumerate(list(self.datasheet_mapping.items())[:5]):
                        status += f"   â€¢ {name}\n"
                    if len(self.datasheet_mapping) > 5:
                        status += f"   â€¢ ... è¿˜æœ‰ {len(self.datasheet_mapping) - 5} ä¸ª\n"
            else:
                status += "ğŸ“Š **æ•°æ®è¡¨åŒæ­¥**: âŒ æœªåŒæ­¥\n"
                status += "ğŸ’¡ **å»ºè®®**: è¿è¡ŒåŒæ­¥åŠŸèƒ½æ¥å‘ç°æ‚¨çš„æ•°æ®è¡¨\n"
            
            # é…ç½®ç‰¹æ€§
            auto_sync = self.plugin_config.get('auto_sync_on_startup', True)
            cache_duration = self.plugin_config.get('cache_duration_hours', 24)
            max_records = self.plugin_config.get('max_records_display', 20)
            
            status += f"\nâš™ï¸ **é…ç½®ä¿¡æ¯**:\n"
            status += f"ğŸ”„ **å¯åŠ¨åŒæ­¥**: {'âœ… å·²å¯ç”¨' if auto_sync else 'âŒ å·²ç¦ç”¨'}\n"
            status += f"ğŸ’¾ **ç¼“å­˜æ—¶é•¿**: {cache_duration} å°æ—¶\n"
            status += f"ğŸ“Š **æœ€å¤§æ˜¾ç¤º**: {max_records} æ¡è®°å½•\n"
            
            # è‡ªå®šä¹‰åˆ«å
            custom_aliases = self.plugin_config.get('custom_aliases', {})
            if custom_aliases:
                status += f"ğŸ·ï¸ **è‡ªå®šä¹‰åˆ«å**: {len(custom_aliases)} ä¸ª\n"
            
            # æ‰‹åŠ¨æ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
            manual_mapping = self.plugin_config.get('datasheet_mapping', {})
            if manual_mapping:
                status += f"ğŸ“ **æ‰‹åŠ¨æ˜ å°„**: {len(manual_mapping)} ä¸ª (å‘åå…¼å®¹)\n"
            
            return status
            
        except Exception as e:
            error_msg = f"âŒ è·å–çŠ¶æ€ä¿¡æ¯å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="upload_vika_attachment")
    async def upload_vika_attachment(
        self, 
        event: AstrMessageEvent, 
        datasheet_name: str, 
        file_path: str,
        record_id: str = None,
        field_name: str = None
    ) -> str:
        """å°†æœ¬åœ°æ–‡ä»¶ä½œä¸ºé™„ä»¶ä¸Šä¼ åˆ°ç»´æ ¼è¡¨ï¼Œå¹¶å¯é€‰æ‹©ç›´æ¥å…³è”åˆ°æŒ‡å®šè®°å½•çš„ç‰¹å®šå­—æ®µã€‚

        Args:
            datasheet_name(string): é™„ä»¶è¦ä¸Šä¼ åˆ°çš„æ•°æ®è¡¨çš„åç§°ã€‚
            file_path(string): è¦ä¸Šä¼ çš„æœ¬åœ°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
            record_id(string): å¯é€‰ï¼Œè¦å°†é™„ä»¶æ·»åŠ åˆ°çš„è®°å½•çš„IDã€‚
            field_name(string): å¯é€‰ï¼Œè¦æ·»åŠ é™„ä»¶çš„å­—æ®µåç§°ã€‚å¦‚æœæä¾›äº† record_idï¼Œæ­¤é¡¹ä¸ºå¿…éœ€ã€‚
        """
        try:
            if not self.vika_client:
                return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"

            if not os.path.exists(file_path):
                return f"âŒ é”™è¯¯ï¼šæ–‡ä»¶æœªæ‰¾åˆ° '{file_path}'"

            if record_id and not field_name:
                return "âŒ é”™è¯¯ï¼šå½“æä¾› record_id æ—¶ï¼Œå¿…é¡»åŒæ—¶æä¾› field_nameã€‚"

            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
            
            datasheet = self._get_datasheet(datasheet_name)
            logger.info(f"å‡†å¤‡å‘æ•°æ®è¡¨ '{datasheet_name}' (ID: {datasheet.dst_id}) ä¸Šä¼ æ–‡ä»¶: {file_path}")

            # æ­¥éª¤ 1: æ‰§è¡Œä¸Šä¼ 
            attachment = await datasheet.attachments.aupload(file_path)
            logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {attachment.name} (Token: {attachment.token})")

            # æ­¥éª¤ 2: å¦‚æœæä¾›äº†è®°å½•IDå’Œå­—æ®µåï¼Œåˆ™ç›´æ¥æ›´æ–°è®°å½•
            if record_id and field_name:
                logger.info(f"å‡†å¤‡å°†é™„ä»¶å…³è”åˆ°è®°å½• '{record_id}' çš„å­—æ®µ '{field_name}'")
                update_data = {
                    'recordId': record_id,
                    'fields': {
                        field_name: [
                            {'token': attachment.token}
                        ]
                    }
                }
                await datasheet.records.aupdate([update_data])
                logger.info(f"æˆåŠŸå°†é™„ä»¶å…³è”åˆ°è®°å½• {record_id}")
                return (
                    f"âœ… æ–‡ä»¶ä¸Šä¼ å¹¶æˆåŠŸå…³è”ï¼\n\n"
                    f"ğŸ“„ **æ–‡ä»¶å**: {attachment.name}\n"
                    f"ğŸ“¦ **å¤§å°**: {attachment.size / 1024:.2f} KB\n"
                    f"ğŸ”— **å·²å…³è”åˆ°**: æ•°æ®è¡¨ '{datasheet_name}' -> è®°å½• '{record_id}' -> å­—æ®µ '{field_name}'"
                )

            # å¦‚æœæ²¡æœ‰æä¾›è®°å½•IDï¼Œåˆ™åªè¿”å›é™„ä»¶ä¿¡æ¯
            result = (
                f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼\n\n"
                f"ğŸ“„ **æ–‡ä»¶å**: {attachment.name}\n"
                f"ğŸ“¦ **å¤§å°**: {attachment.size / 1024:.2f} KB\n"
                f"ğŸ”— **URL**: {attachment.url}\n"
                f"ğŸ”‘ **é™„ä»¶Token**: `{attachment.token}`\n\n"
                f"ğŸ’¡ **æç¤º**: æ‚¨å¯ä»¥åœ¨'æ·»åŠ è®°å½•'æˆ–'æ›´æ–°è®°å½•'æ—¶ï¼Œåœ¨ç›¸åº”çš„é™„ä»¶å­—æ®µä¸­ä½¿ç”¨æ­¤é™„ä»¶Tokenã€‚"
            )
            return result

        except Exception as e:
            error_msg = f"âŒ ä¸Šä¼ é™„ä»¶åˆ° '{datasheet_name}' å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    async def initialize(self):
        """æ’ä»¶åˆå§‹åŒ–"""
        logger.info("ç»´æ ¼è¡¨MCPæ’ä»¶åˆå§‹åŒ–å®Œæˆ")
        # å¯åŠ¨æ—¶ä¸å†è‡ªåŠ¨åŒæ­¥ï¼Œæ”¹ä¸ºæŒ‰éœ€åŒæ­¥
        # asyncio.create_task(self._auto_sync_if_needed())

    async def terminate(self):
        """æ’ä»¶é”€æ¯"""
        logger.info("ç»´æ ¼è¡¨MCPæ’ä»¶å·²é”€æ¯")
