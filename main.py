import asyncio
import json
import traceback
import os
import time
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timedelta

from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api import logger, AstrBotConfig

try:
    from astral_vika import Vika
except ImportError:
    logger.error("ç»´æ ¼è¡¨MCPæ’ä»¶å¯åŠ¨å¤±è´¥ï¼šæœªæ‰¾åˆ° astral_vika åº“ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–å®‰è£…")
    Vika = None


@register("vika_mcp_plugin", "AstralSolipsism", "æ™ºèƒ½ç»´æ ¼è¡¨MCPæ’ä»¶ï¼Œæ”¯æŒè‡ªåŠ¨å‘ç°æ•°æ®è¡¨ï¼Œå¤§æ¨¡å‹è‡ªåŠ¨è°ƒç”¨å„ç§æ“ä½œåŠŸèƒ½", "0.9")
class VikaMcpPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config
        self.vika_client = None
        self.plugin_config = {}
        
        # æ™ºèƒ½æ•°æ®è¡¨ç®¡ç†
        self.datasheet_mapping = {}  # è‡ªåŠ¨å‘ç°çš„æ•°æ®è¡¨æ˜ å°„ {name: id}
        self.spaces_list = []  # ç©ºé—´ç«™åˆ—è¡¨
        self.default_space_id = None
        self.cache_file_path = os.path.join(os.getcwd(), "data", "vika_cache.json")
        self.cache_timestamp = None
        self.is_synced = False
        
        # åŠ è½½æ’ä»¶é…ç½®
        self._load_config()
        
        # åˆå§‹åŒ–ç»´æ ¼è¡¨å®¢æˆ·ç«¯
        self._init_vika_client()
        
        # åŠ è½½ç¼“å­˜
        self._load_cache()

    def _load_config(self):
        """åŠ è½½æ’ä»¶é…ç½®"""
        try:
            plugin_name = "vika_mcp_plugin"
            self.plugin_config = self.config.get_plugin_config(plugin_name) or {}
            self.default_space_id = self.plugin_config.get('default_space_id', '')
            logger.info(f"ç»´æ ¼è¡¨MCPæ’ä»¶é…ç½®åŠ è½½å®Œæˆ: {list(self.plugin_config.keys())}")
        except Exception as e:
            logger.error(f"åŠ è½½ç»´æ ¼è¡¨MCPæ’ä»¶é…ç½®å¤±è´¥: {e}")
            self.plugin_config = {}

    def _init_vika_client(self):
        """åˆå§‹åŒ–ç»´æ ¼è¡¨å®¢æˆ·ç«¯"""
        if not Vika:
            logger.error("ç»´æ ¼è¡¨MCPæ’ä»¶åˆå§‹åŒ–å¤±è´¥ï¼švika åº“æœªå®‰è£…")
            return
            
        api_token = self.plugin_config.get('vika_api_token', '')
        vika_host = self.plugin_config.get('vika_host', 'https://api.vika.cn')
        
        if not api_token:
            logger.warning("ç»´æ ¼è¡¨ API Token æœªé…ç½®")
            return
            
        try:
            # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œæ”¯æŒè‡ªå®šä¹‰host
            self.vika_client = Vika(api_token, host=vika_host)
            logger.info(f"ç»´æ ¼è¡¨å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒæœåŠ¡å™¨: {vika_host}")
        except Exception as e:
            logger.error(f"ç»´æ ¼è¡¨å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

    def _load_cache(self):
        """åŠ è½½ç¼“å­˜çš„æ•°æ®è¡¨ä¿¡æ¯"""
        try:
            if os.path.exists(self.cache_file_path):
                with open(self.cache_file_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    
                self.datasheet_mapping = cache_data.get('datasheet_mapping', {})
                self.spaces_list = cache_data.get('spaces_list', [])
                self.cache_timestamp = cache_data.get('timestamp', 0)
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                cache_duration = self.plugin_config.get('cache_duration_hours', 24)
                if time.time() - self.cache_timestamp < cache_duration * 3600:
                    self.is_synced = True
                    logger.info(f"ä»ç¼“å­˜åŠ è½½äº† {len(self.datasheet_mapping)} ä¸ªæ•°æ®è¡¨")
                else:
                    logger.info("ç¼“å­˜å·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°åŒæ­¥")
            else:
                logger.info("æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œéœ€è¦åˆå§‹åŒæ­¥")
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(self.cache_file_path), exist_ok=True)
            cache_data = {
                'datasheet_mapping': self.datasheet_mapping,
                'spaces_list': self.spaces_list,
                'timestamp': time.time()
            }
            with open(self.cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logger.info("ç¼“å­˜å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    async def _run_sync_in_thread(self, func, *args, **kwargs):
        """åœ¨çº¿ç¨‹æ± ä¸­å¼‚æ­¥æ‰§è¡ŒåŒæ­¥å‡½æ•°"""
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, func, *args, **kwargs)

    def _get_datasheet_id(self, datasheet_name: str) -> str:
        """æ™ºèƒ½è·å–æ•°æ®è¡¨ID"""
        # 1. é¦–å…ˆæ£€æŸ¥è‡ªåŠ¨å‘ç°çš„æ˜ å°„
        if datasheet_name in self.datasheet_mapping:
            return self.datasheet_mapping[datasheet_name]
            
        # 2. æ£€æŸ¥è‡ªå®šä¹‰åˆ«å
        custom_aliases = self.plugin_config.get('custom_aliases', {})
        if datasheet_name in custom_aliases:
            real_name = custom_aliases[datasheet_name]
            if real_name in self.datasheet_mapping:
                return self.datasheet_mapping[real_name]
                
        # 3. æ£€æŸ¥æ‰‹åŠ¨é…ç½®çš„æ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
        manual_mapping = self.plugin_config.get('datasheet_mapping', {})
        if datasheet_name in manual_mapping:
            return manual_mapping[datasheet_name]
            
        # 4. å¦‚æœæ˜¯ç›´æ¥çš„datasheet IDæ ¼å¼ï¼Œç›´æ¥è¿”å›
        if datasheet_name.startswith('dst'):
            return datasheet_name
            
        # 5. æ¨¡ç³ŠåŒ¹é…ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
        for name, ds_id in self.datasheet_mapping.items():
            if datasheet_name.lower() in name.lower() or name.lower() in datasheet_name.lower():
                return ds_id
                
        # 6. éƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸåç§°
        return datasheet_name

    def _get_datasheet(self, datasheet_name: str):
        """è·å–æ•°æ®è¡¨å¯¹è±¡"""
        if not self.vika_client:
            raise ValueError("ç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokenå’ŒæœåŠ¡å™¨é…ç½®")
        
        datasheet_id = self._get_datasheet_id(datasheet_name)
        if not datasheet_id:
            raise ValueError(f"æœªæ‰¾åˆ°æ•°æ®è¡¨: {datasheet_name}")
        
        return self.vika_client.datasheet(datasheet_id)
        
    async def _traverse_node_recursive(self, node_id: str, datasheet_map: Dict[str, str], depth: int = 0):
        """é€’å½’éå†èŠ‚ç‚¹ï¼Œå‘ç°æ‰€æœ‰æ•°æ®è¡¨"""
        if depth > 10:  # é˜²æ­¢æ— é™é€’å½’
            logger.warning(f"é€’å½’æ·±åº¦è¶…è¿‡é™åˆ¶ï¼Œåœæ­¢éå†èŠ‚ç‚¹: {node_id}")
            return
            
        try:
            node_detail = await self._run_sync_in_thread(
                lambda: self.vika_client.nodes.get(node_id)
            )
            
            if hasattr(node_detail, 'type'):
                if node_detail.type == 'Datasheet':
                    datasheet_map[node_detail.name] = node_detail.id
                    logger.info(f"å‘ç°æ•°æ®è¡¨: {node_detail.name} ({node_detail.id})")
                elif node_detail.type == 'Folder' and hasattr(node_detail, 'children'):
                    for child in node_detail.children:
                        await self._traverse_node_recursive(child.get('id'), datasheet_map, depth + 1)
        except Exception as e:
            logger.error(f"éå†èŠ‚ç‚¹ {node_id} æ—¶å‡ºé”™: {e}")

    async def _auto_sync_if_needed(self):
        """å¦‚æœéœ€è¦ä¸”é…ç½®å…è®¸ï¼Œè‡ªåŠ¨åŒæ­¥æ•°æ®è¡¨"""
        if (not self.is_synced and 
            self.plugin_config.get('auto_sync_on_startup', True) and 
            self.vika_client):
            try:
                await self._perform_sync()
            except Exception as e:
                logger.error(f"è‡ªåŠ¨åŒæ­¥å¤±è´¥: {e}")

    async def _perform_sync(self):
        """æ‰§è¡Œæ•°æ®è¡¨åŒæ­¥"""
        if not self.vika_client:
            raise ValueError("ç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
        # è·å–ç©ºé—´ç«™åˆ—è¡¨
        spaces = await self._run_sync_in_thread(
            lambda: list(self.vika_client.spaces.all())
        )
        
        if not spaces:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•ç©ºé—´ç«™ï¼Œè¯·æ£€æŸ¥API Tokenæƒé™")
            
        self.spaces_list = [{'id': space.id, 'name': space.name} for space in spaces]
        
        # ç¡®å®šè¦åŒæ­¥çš„ç©ºé—´ç«™
        target_space = None
        if self.default_space_id:
            target_space = next((s for s in spaces if s.id == self.default_space_id), None)
        
        if not target_space:
            target_space = spaces[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç©ºé—´ç«™
            
        logger.info(f"åŒæ­¥ç©ºé—´ç«™: {target_space.name} ({target_space.id})")
        
        # è·å–æ ¹èŠ‚ç‚¹å¹¶é€’å½’éå†
        root_nodes = await self._run_sync_in_thread(
            lambda: list(self.vika_client.space(target_space.id).nodes.all())
        )
        
        new_datasheet_map = {}
        for node in root_nodes:
            await self._traverse_node_recursive(node.id, new_datasheet_map)
            
        self.datasheet_mapping.update(new_datasheet_map)
        self.is_synced = True
        self.cache_timestamp = time.time()
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_cache()
        
        return len(new_datasheet_map)

    def _format_records_for_display(self, records: List[Dict[str, Any]], limit: int = None) -> str:
        """æ ¼å¼åŒ–è®°å½•ä¸ºå¯è¯»çš„æ–‡æœ¬æ ¼å¼"""
        if not records:
            return "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®°å½•"

        max_display = limit or self.plugin_config.get('max_records_display', 20)
        display_records = records[:max_display]
        
        # æ„å»ºè¡¨æ ¼æ˜¾ç¤º
        result = f"æ‰¾åˆ° {len(records)} æ¡è®°å½•"
        if len(records) > max_display:
            result += f"ï¼ˆæ˜¾ç¤ºå‰ {max_display} æ¡ï¼‰"
        result += ":\\n\\n"
        
        if display_records:
            # è·å–æ‰€æœ‰å­—æ®µå
            all_fields = set()
            for record in display_records:
                if 'fields' in record:
                    all_fields.update(record['fields'].keys())
            
            field_list = list(all_fields)
            
            # è¡¨å¤´
            result += "| " + " | ".join(field_list) + " |\\n"
            result += "|" + "---|" * len(field_list) + "\\n"
            
            # æ•°æ®è¡Œ
            for record in display_records:
                fields = record.get('fields', {})
                row_data = []
                for field in field_list:
                    value = fields.get(field, '')
                    # å¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œé•¿æ–‡æœ¬
                    if isinstance(value, str):
                        value = value.replace('|', '\\|').replace('\\n', ' ')
                        if len(value) > 30:
                            value = value[:30] + "..."
                    elif value is None:
                        value = ""
                    row_data.append(str(value))
                result += "| " + " | ".join(row_data) + " |\\n"
            
        return result

    def _parse_field_data(self, field_data_str: str) -> Dict[str, Any]:
        """è§£æå­—æ®µæ•°æ®å­—ç¬¦ä¸²ä¸ºå­—å…¸"""
        try:
            # å°è¯•è§£æJSONæ ¼å¼
            return json.loads(field_data_str)
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯JSONï¼Œå°è¯•è§£æé”®å€¼å¯¹æ ¼å¼
            fields = {}
            pairs = field_data_str.split(',')
            for pair in pairs:
                if '=' in pair:
                    key, value = pair.split('=', 1)
                    key = key.strip().strip('"').strip("'")
                    value = value.strip().strip('"').strip("'")
                    
                    # è‡ªåŠ¨ç±»å‹è½¬æ¢
                    if self.plugin_config.get('enable_auto_type_conversion', True):
                        if value.isdigit():
                            value = int(value)
                        elif value.replace('.', '').isdigit():
                            value = float(value)
                        elif value.lower() in ['true', 'false']:
                            value = value.lower() == 'true'
                    
                    fields[key] = value
            return fields

    @filter.on_llm_request()
    async def on_startup_sync(self, event):
        """æ’ä»¶å¯åŠ¨æ—¶è‡ªåŠ¨åŒæ­¥"""
        await self._auto_sync_if_needed()

    @filter.llm_tool(name="sync_vika_datasheets")
    async def sync_vika_datasheets(self, event: AstrMessageEvent) -> str:
        """åŒæ­¥å¹¶ç¼“å­˜ç»´æ ¼ç©ºé—´ç«™ä¸­çš„æ‰€æœ‰æ•°æ®è¡¨ï¼Œè®©æ‚¨å¯ä»¥é€šè¿‡åç§°ç›´æ¥æ“ä½œå®ƒä»¬ã€‚
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        try:
            discovered_count = await self._perform_sync()
            
            result = f"âœ… åŒæ­¥å®Œæˆï¼å…±å‘ç° {discovered_count} ä¸ªæ•°æ®è¡¨ã€‚\n\n"
            result += "ğŸ“‹ **å·²å‘ç°çš„æ•°æ®è¡¨**ï¼š\n"
            
            for name, ds_id in list(self.datasheet_mapping.items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                result += f"â€¢ {name} (`{ds_id}`)\n"
                
            if len(self.datasheet_mapping) > 10:
                result += f"â€¢ ... è¿˜æœ‰ {len(self.datasheet_mapping) - 10} ä¸ªæ•°æ®è¡¨\n"
                
            result += "\nğŸ’¡ ç°åœ¨æ‚¨å¯ä»¥ç›´æ¥é€šè¿‡æ•°æ®è¡¨åç§°æ¥æ“ä½œå®ƒä»¬äº†ï¼"
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ åŒæ­¥ç»´æ ¼è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="list_vika_spaces")
    async def list_vika_spaces(self, event: AstrMessageEvent) -> str:
        """åˆ—å‡ºæ‚¨æœ‰æƒè®¿é—®çš„æ‰€æœ‰ç»´æ ¼è¡¨ç©ºé—´ç«™ã€‚
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        try:
            spaces = await self._run_sync_in_thread(
                lambda: list(self.vika_client.spaces.all())
            )
            
            if not spaces:
                return "ğŸ“­ æœªæ‰¾åˆ°ä»»ä½•ç©ºé—´ç«™ï¼Œè¯·æ£€æŸ¥æ‚¨çš„API Tokenæƒé™"
                
            result = f"ğŸ¢ **æ‚¨çš„ç»´æ ¼è¡¨ç©ºé—´ç«™** (å…± {len(spaces)} ä¸ª)ï¼š\n\n"
            
            for space in spaces:
                is_default = " ğŸ”¸ *é»˜è®¤*" if space.id == self.default_space_id else ""
                result += f"â€¢ **{space.name}**{is_default}\n"
                result += f"  ID: `{space.id}`\n\n"
                
            if not self.default_space_id and len(spaces) > 1:
                result += "ğŸ’¡ **æç¤º**: å¦‚æœæ‚¨æœ‰å¤šä¸ªç©ºé—´ç«™ï¼Œå»ºè®®åœ¨é…ç½®ä¸­è®¾ç½® `default_space_id` ä»¥æŒ‡å®šé»˜è®¤æ“ä½œçš„ç©ºé—´ç«™ã€‚"
                
            return result
            
        except Exception as e:
            error_msg = f"âŒ è·å–ç©ºé—´ç«™åˆ—è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="list_vika_datasheets")
    async def list_vika_datasheets(self, event: AstrMessageEvent, filter_keyword: str = None) -> str:
        """åˆ—å‡ºæ‰€æœ‰å·²å‘ç°çš„ç»´æ ¼è¡¨æ•°æ®è¡¨ï¼Œæ”¯æŒå…³é”®è¯è¿‡æ»¤ã€‚

        Args:
            filter_keyword (str): å¯é€‰ï¼Œè¿‡æ»¤å…³é”®è¯ï¼Œåªæ˜¾ç¤ºåŒ…å«è¯¥å…³é”®è¯çš„æ•°æ®è¡¨
        """
        if not self.is_synced:
            return "âš ï¸ æ•°æ®è¡¨åˆ—è¡¨å°šæœªåŒæ­¥ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®è¡¨åŒæ­¥åŠŸèƒ½ï¼Œæˆ–è€…ç¨ç­‰ç‰‡åˆ»è®©ç³»ç»Ÿè‡ªåŠ¨åŒæ­¥ã€‚"
            
        if not self.datasheet_mapping:
            return "ğŸ“­ æœªå‘ç°ä»»ä½•æ•°æ®è¡¨ï¼Œè¯·æ£€æŸ¥ç©ºé—´ç«™ä¸­æ˜¯å¦æœ‰æ•°æ®è¡¨ï¼Œæˆ–é‡æ–°åŒæ­¥ã€‚"
            
        # åº”ç”¨è¿‡æ»¤
        filtered_tables = self.datasheet_mapping
        if filter_keyword:
            filtered_tables = {
                name: ds_id for name, ds_id in self.datasheet_mapping.items()
                if filter_keyword.lower() in name.lower()
            }
            
        if not filtered_tables:
            return f"ğŸ” æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ '{filter_keyword}' çš„æ•°æ®è¡¨ã€‚"
            
        result = f"ğŸ“Š **æ•°æ®è¡¨åˆ—è¡¨**"
        if filter_keyword:
            result += f" (åŒ…å« '{filter_keyword}')"
        result += f" (å…± {len(filtered_tables)} ä¸ª)ï¼š\n\n"
        
        for name, ds_id in filtered_tables.items():
            result += f"â€¢ **{name}**\n"
            result += f"  ID: `{ds_id}`\n\n"
            
        # æ˜¾ç¤ºè‡ªå®šä¹‰åˆ«åæç¤º
        custom_aliases = self.plugin_config.get('custom_aliases', {})
        if custom_aliases:
            result += "ğŸ·ï¸ **è‡ªå®šä¹‰åˆ«å**ï¼š\n"
            for alias, real_name in custom_aliases.items():
                if real_name in self.datasheet_mapping:
                    result += f"â€¢ `{alias}` â†’ {real_name}\n"
            result += "\n"
            
        result += "ğŸ’¡ **æç¤º**: æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ•°æ®è¡¨åç§°è¿›è¡Œæ“ä½œï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«ã€‚"
        
        return result

    @filter.llm_tool(name="create_vika_datasheet")
    async def create_vika_datasheet(
        self, 
        event: AstrMessageEvent, 
        name: str, 
        description: str = "",
        fields_config: str = ""
    ) -> str:
        """åœ¨ç»´æ ¼è¡¨ç©ºé—´ç«™ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®è¡¨ã€‚

        Args:
            name (str): æ•°æ®è¡¨åç§°ï¼ˆå¿…éœ€ï¼‰
            description (str): æ•°æ®è¡¨æè¿°ï¼ˆå¯é€‰ï¼‰
            fields_config (str): å­—æ®µé…ç½®ï¼ŒJSONæ ¼å¼ï¼Œä¾‹å¦‚ï¼š'[{"name":"å§“å","type":"SingleLineText"},{"name":"å¹´é¾„","type":"Number"}]'
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        try:
            # ç¡®å®šç›®æ ‡ç©ºé—´ç«™
            spaces = await self._run_sync_in_thread(
                lambda: list(self.vika_client.spaces.all())
            )
            
            if not spaces:
                return "âŒ æœªæ‰¾åˆ°ä»»ä½•ç©ºé—´ç«™ï¼Œè¯·æ£€æŸ¥API Tokenæƒé™"
                
            target_space = None
            if self.default_space_id:
                target_space = next((s for s in spaces if s.id == self.default_space_id), None)
            
            if not target_space:
                target_space = spaces[0]
                
            # è§£æå­—æ®µé…ç½®
            fields = []
            if fields_config:
                try:
                    fields = json.loads(fields_config)
                except json.JSONDecodeError:
                    return "âŒ å­—æ®µé…ç½®æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨æ­£ç¡®çš„JSONæ ¼å¼"
                    
            # åˆ›å»ºæ•°æ®è¡¨
            create_params = {
                'name': name,
                'description': description,
                'folderId': target_space.id
            }
            
            if fields:
                create_params['fields'] = fields
                
            new_datasheet = await self._run_sync_in_thread(
                lambda: self.vika_client.space(target_space.id).datasheets.create(**create_params)
            )
            
            # æ›´æ–°æœ¬åœ°ç¼“å­˜
            self.datasheet_mapping[name] = new_datasheet.id
            self._save_cache()
            
            result = f"âœ… æ•°æ®è¡¨åˆ›å»ºæˆåŠŸï¼\n\n"
            result += f"ğŸ“Š **æ•°æ®è¡¨åç§°**: {name}\n"
            result += f"ğŸ†” **æ•°æ®è¡¨ID**: `{new_datasheet.id}`\n"
            result += f"ğŸ¢ **æ‰€åœ¨ç©ºé—´ç«™**: {target_space.name}\n"
            
            if description:
                result += f"ğŸ“ **æè¿°**: {description}\n"
                
            if fields:
                result += f"ğŸ“‹ **å­—æ®µæ•°é‡**: {len(fields)} ä¸ª\n"
                
            result += "\nğŸ’¡ æ•°æ®è¡¨å·²æ·»åŠ åˆ°ç¼“å­˜ï¼Œæ‚¨å¯ä»¥ç›´æ¥é€šè¿‡åç§°æ“ä½œå®ƒã€‚"
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ åˆ›å»ºæ•°æ®è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    async def _check_sync_and_suggest(self, datasheet_name: str) -> Optional[str]:
        """æ£€æŸ¥æ•°æ®è¡¨æ˜¯å¦åŒæ­¥ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™æä¾›å»ºè®®"""
        if not self.vika_client:
            return "âŒ ç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        # å°è¯•æ™ºèƒ½è·å–æ•°æ®è¡¨ID
        datasheet_id = self._get_datasheet_id(datasheet_name)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®è¡¨ï¼Œæä¾›å»ºè®®
        if datasheet_id == datasheet_name and not datasheet_name.startswith('dst'):
            if not self.is_synced:
                return f"âš ï¸ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}'ï¼Œä¸”æ•°æ®è¡¨åˆ—è¡¨å°šæœªåŒæ­¥ã€‚è¯·å…ˆè¿è¡Œæ•°æ®è¡¨åŒæ­¥åŠŸèƒ½ã€‚"
            else:
                # æä¾›ç›¸ä¼¼çš„æ•°æ®è¡¨å»ºè®®
                similar_tables = []
                for table_name in self.datasheet_mapping.keys():
                    if any(word in table_name.lower() for word in datasheet_name.lower().split()):
                        similar_tables.append(table_name)
                
                if similar_tables:
                    suggestion = f"âŒ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}'ã€‚æ‚¨æ˜¯å¦æƒ³è¦æ“ä½œï¼š\n"
                    for table in similar_tables[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªå»ºè®®
                        suggestion += f"â€¢ {table}\n"
                    return suggestion
                else:
                    available_tables = list(self.datasheet_mapping.keys())[:5]
                    return (f"âŒ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}'ã€‚\n"
                           f"å¯ç”¨çš„æ•°æ®è¡¨ï¼š{', '.join(available_tables)}")
        
        return None  # æ‰¾åˆ°äº†æ•°æ®è¡¨ï¼Œæ— éœ€æç¤º

    @filter.llm_tool(name="get_vika_records")
    async def get_vika_records(
        self, 
        event: AstrMessageEvent, 
        datasheet_name: str, 
        view_name: str = None,
        filter_formula: str = None,
        max_records: int = None
    ) -> str:
        """ä»æŒ‡å®šçš„ç»´æ ¼è¡¨ä¸­è·å–è®°å½•æ•°æ®ã€‚

        Args:
            datasheet_name (str): è¦æŸ¥è¯¢çš„æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
            view_name (str): å¯é€‰ï¼ŒæŒ‡å®šè§†å›¾åç§°
            filter_formula (str): å¯é€‰ï¼Œè¿‡æ»¤å…¬å¼ï¼Œç”¨äºç­›é€‰è®°å½•
            max_records (int): å¯é€‰ï¼Œæœ€å¤§è¿”å›è®°å½•æ•°ï¼Œé»˜è®¤ä¸ºé…ç½®ä¸­çš„å€¼
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # å‡†å¤‡æŸ¥è¯¢å‚æ•°
            query_params = {}
            if view_name:
                query_params['view'] = view_name
            if filter_formula:
                query_params['filterByFormula'] = filter_formula
            if max_records:
                query_params['maxRecords'] = min(max_records, 100)  # APIé™åˆ¶
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ“ä½œ
            if query_params:
                records = await self._run_sync_in_thread(
                    lambda: list(datasheet.records.filter(**query_params))
                )
            else:
                records = await self._run_sync_in_thread(
                    lambda: list(datasheet.records.all())
                )
            
            return self._format_records_for_display(records, max_records)
            
        except Exception as e:
            error_msg = f"âŒ è·å–ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="add_vika_record")
    async def add_vika_record(
        self, 
        event: AstrMessageEvent, 
        datasheet_name: str, 
        record_data: str
    ) -> str:
        """å‘æŒ‡å®šçš„ç»´æ ¼è¡¨ä¸­æ·»åŠ æ–°è®°å½•ã€‚

        Args:
            datasheet_name (str): è¦æ·»åŠ è®°å½•çš„æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
            record_data (str): è®°å½•æ•°æ®ï¼Œå¯ä»¥æ˜¯JSONæ ¼å¼æˆ–é”®å€¼å¯¹æ ¼å¼ï¼ˆå¦‚ï¼š"å§“å=å¼ ä¸‰,å¹´é¾„=25"æˆ–'{"å§“å":"å¼ ä¸‰","å¹´é¾„":25}'ï¼‰
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # è§£æè®°å½•æ•°æ®
            fields_data = self._parse_field_data(record_data)
            
            if not fields_data:
                return "âŒ é”™è¯¯ï¼šè®°å½•æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚è¯·ä½¿ç”¨JSONæ ¼å¼æˆ–é”®å€¼å¯¹æ ¼å¼ï¼ˆå¦‚ï¼š'å§“å=å¼ ä¸‰,å¹´é¾„=25'ï¼‰"
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ“ä½œ
            result = await self._run_sync_in_thread(
                lambda: datasheet.records.create(fields_data)
            )
            
            return f"âœ… æˆåŠŸæ·»åŠ è®°å½•åˆ°æ•°æ®è¡¨ '{datasheet_name}'ï¼Œè®°å½•ID: {result.record_id}"
            
        except Exception as e:
            error_msg = f"âŒ æ·»åŠ ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="search_vika_records")
    async def search_vika_records(
        self, 
        event: AstrMessageEvent, 
        datasheet_name: str, 
        search_query: str,
        search_fields: str = None
    ) -> str:
        """åœ¨æŒ‡å®šçš„ç»´æ ¼è¡¨ä¸­æœç´¢åŒ…å«ç‰¹å®šå†…å®¹çš„è®°å½•ã€‚

        Args:
            datasheet_name (str): è¦æœç´¢çš„æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
            search_query (str): æœç´¢å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰
            search_fields (str): å¯é€‰ï¼ŒæŒ‡å®šè¦æœç´¢çš„å­—æ®µåï¼Œå¤šä¸ªå­—æ®µç”¨é€—å·åˆ†éš”
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # è·å–æ‰€æœ‰è®°å½•
            all_records = await self._run_sync_in_thread(
                lambda: list(datasheet.records.all())
            )
            
            # æ‰§è¡Œæœç´¢
            matching_records = []
            query_lower = search_query.lower()
            search_field_list = None
            
            if search_fields:
                search_field_list = [f.strip() for f in search_fields.split(',')]
            
            for record in all_records:
                fields = record.get('fields', {})
                
                # ç¡®å®šè¦æœç´¢çš„å­—æ®µ
                fields_to_search = search_field_list if search_field_list else fields.keys()
                
                # åœ¨æŒ‡å®šå­—æ®µä¸­æœç´¢
                for field_name in fields_to_search:
                    if field_name in fields:
                        field_value = fields[field_name]
                        if isinstance(field_value, str) and query_lower in field_value.lower():
                            matching_records.append(record)
                            break
                        elif str(field_value).lower() == query_lower:
                            matching_records.append(record)
                            break
            
            if not matching_records:
                search_scope = f"åœ¨å­—æ®µ [{search_fields}] ä¸­" if search_fields else "åœ¨æ‰€æœ‰å­—æ®µä¸­"
                return f"ğŸ” {search_scope}æ²¡æœ‰æ‰¾åˆ°åŒ…å« '{search_query}' çš„è®°å½•"
            
            result = f"ğŸ” **æœç´¢ç»“æœ** (å…³é”®è¯: '{search_query}'):\n\n"
            result += self._format_records_for_display(matching_records)
            return result
            
        except Exception as e:
            error_msg = f"âŒ æœç´¢ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="update_vika_record")
    async def update_vika_record(
        self, 
        event: AstrMessageEvent, 
        datasheet_name: str, 
        record_id: str,
        update_data: str
    ) -> str:
        """æ›´æ–°æŒ‡å®šç»´æ ¼è¡¨ä¸­çš„è®°å½•ã€‚

        Args:
            datasheet_name (str): æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
            record_id (str): è¦æ›´æ–°çš„è®°å½•IDï¼ˆå¿…éœ€ï¼‰
            update_data (str): æ›´æ–°æ•°æ®ï¼Œæ ¼å¼åŒæ·»åŠ è®°å½•
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # è§£ææ›´æ–°æ•°æ®
            fields_data = self._parse_field_data(update_data)
            
            if not fields_data:
                return "âŒ é”™è¯¯ï¼šæ›´æ–°æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®"
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ“ä½œ
            await self._run_sync_in_thread(
                lambda: datasheet.records.update(record_id, fields_data)
            )
            
            return f"âœ… æˆåŠŸæ›´æ–°æ•°æ®è¡¨ '{datasheet_name}' ä¸­çš„è®°å½• {record_id}"
            
        except Exception as e:
            error_msg = f"âŒ æ›´æ–°ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="delete_vika_record")
    async def delete_vika_record(
        self, 
        event: AstrMessageEvent, 
        datasheet_name: str, 
        record_id: str
    ) -> str:
        """åˆ é™¤æŒ‡å®šç»´æ ¼è¡¨ä¸­çš„è®°å½•ã€‚

        Args:
            datasheet_name (str): æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
            record_id (str): è¦åˆ é™¤çš„è®°å½•IDï¼ˆå¿…éœ€ï¼‰
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥æ“ä½œ
            await self._run_sync_in_thread(
                lambda: datasheet.records.delete(record_id)
            )
            
            return f"âœ… æˆåŠŸåˆ é™¤æ•°æ®è¡¨ '{datasheet_name}' ä¸­çš„è®°å½• {record_id}"
            
        except Exception as e:
            error_msg = f"âŒ åˆ é™¤ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="get_vika_fields")
    async def get_vika_fields(
        self, 
        event: AstrMessageEvent, 
        datasheet_name: str
    ) -> str:
        """è·å–æŒ‡å®šç»´æ ¼è¡¨çš„å­—æ®µä¿¡æ¯ã€‚

        Args:
            datasheet_name (str): æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # è·å–å­—æ®µä¿¡æ¯
            fields = await self._run_sync_in_thread(
                lambda: list(datasheet.fields.all())
            )
            
            if not fields:
                return f"ğŸ“‹ æ•°æ®è¡¨ '{datasheet_name}' æ²¡æœ‰å­—æ®µä¿¡æ¯"
            
            result = f"ğŸ“‹ **æ•°æ®è¡¨ '{datasheet_name}' çš„å­—æ®µä¿¡æ¯** (å…± {len(fields)} ä¸ªå­—æ®µ):\n\n"
            for field in fields:
                field_name = field.get('name', 'æœªçŸ¥')
                field_type = field.get('type', 'æœªçŸ¥')
                field_desc = field.get('description', '')
                
                result += f"â€¢ **{field_name}** (ç±»å‹: {field_type})"
                if field_desc:
                    result += f" - {field_desc}"
                result += "\n"
            
            result += "\nğŸ’¡ æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›å­—æ®µåæ¥æ·»åŠ æˆ–æ›´æ–°è®°å½•ã€‚"
            return result
            
        except Exception as e:
            error_msg = f"âŒ è·å–ç»´æ ¼è¡¨å­—æ®µä¿¡æ¯å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="get_vika_status")
    async def get_vika_status(self, event: AstrMessageEvent) -> str:
        """æ£€æŸ¥ç»´æ ¼è¡¨æ’ä»¶çš„è¿æ¥çŠ¶æ€ã€é…ç½®ä¿¡æ¯å’Œæ•°æ®è¡¨åŒæ­¥çŠ¶æ€ã€‚
        """
        try:
            status = "ğŸ”§ **ç»´æ ¼è¡¨MCPæ’ä»¶çŠ¶æ€**\n\n"
            
            # æ£€æŸ¥å®¢æˆ·ç«¯çŠ¶æ€
            if self.vika_client:
                status += "âœ… **å®¢æˆ·ç«¯è¿æ¥**: æ­£å¸¸\n"
            else:
                status += "âŒ **å®¢æˆ·ç«¯è¿æ¥**: æœªå»ºç«‹\n"
            
            # æ£€æŸ¥é…ç½®
            api_token = self.plugin_config.get('vika_api_token', '')
            vika_host = self.plugin_config.get('vika_host', 'https://api.vika.cn')
            
            if api_token:
                status += f"ğŸ”‘ **API Token**: å·²é…ç½® ({api_token[:8]}...)\n"
            else:
                status += "ğŸ”‘ **API Token**: âŒ æœªé…ç½®\n"
            
            status += f"ğŸŒ **æœåŠ¡å™¨åœ°å€**: {vika_host}\n"
            
            # ç©ºé—´ç«™ä¿¡æ¯
            if self.spaces_list:
                status += f"ğŸ¢ **ç©ºé—´ç«™**: å…± {len(self.spaces_list)} ä¸ª\n"
                if self.default_space_id:
                    default_space = next((s for s in self.spaces_list if s['id'] == self.default_space_id), None)
                    if default_space:
                        status += f"ğŸ”¸ **é»˜è®¤ç©ºé—´ç«™**: {default_space['name']}\n"
            else:
                status += "ğŸ¢ **ç©ºé—´ç«™**: æœªåŒæ­¥\n"
            
            # æ•°æ®è¡¨åŒæ­¥çŠ¶æ€
            if self.is_synced:
                cache_age = int((time.time() - self.cache_timestamp) / 3600) if self.cache_timestamp else 0
                status += f"ğŸ“Š **æ•°æ®è¡¨åŒæ­¥**: âœ… å·²åŒæ­¥ ({len(self.datasheet_mapping)} ä¸ªè¡¨)\n"
                status += f"ğŸ• **ç¼“å­˜æ—¶é—´**: {cache_age} å°æ—¶å‰\n"
                
                # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®è¡¨
                if self.datasheet_mapping:
                    status += "ğŸ“‹ **éƒ¨åˆ†æ•°æ®è¡¨**:\n"
                    for i, (name, _) in enumerate(list(self.datasheet_mapping.items())[:5]):
                        status += f"   â€¢ {name}\n"
                    if len(self.datasheet_mapping) > 5:
                        status += f"   â€¢ ... è¿˜æœ‰ {len(self.datasheet_mapping) - 5} ä¸ª\n"
            else:
                status += "ğŸ“Š **æ•°æ®è¡¨åŒæ­¥**: âŒ æœªåŒæ­¥\n"
                status += "ğŸ’¡ **å»ºè®®**: è¿è¡ŒåŒæ­¥åŠŸèƒ½æ¥å‘ç°æ‚¨çš„æ•°æ®è¡¨\n"
            
            # é…ç½®ç‰¹æ€§
            auto_sync = self.plugin_config.get('auto_sync_on_startup', True)
            cache_duration = self.plugin_config.get('cache_duration_hours', 24)
            max_records = self.plugin_config.get('max_records_display', 20)
            
            status += f"\nâš™ï¸ **é…ç½®ä¿¡æ¯**:\n"
            status += f"ğŸ”„ **å¯åŠ¨åŒæ­¥**: {'âœ… å·²å¯ç”¨' if auto_sync else 'âŒ å·²ç¦ç”¨'}\n"
            status += f"ğŸ’¾ **ç¼“å­˜æ—¶é•¿**: {cache_duration} å°æ—¶\n"
            status += f"ğŸ“Š **æœ€å¤§æ˜¾ç¤º**: {max_records} æ¡è®°å½•\n"
            
            # è‡ªå®šä¹‰åˆ«å
            custom_aliases = self.plugin_config.get('custom_aliases', {})
            if custom_aliases:
                status += f"ğŸ·ï¸ **è‡ªå®šä¹‰åˆ«å**: {len(custom_aliases)} ä¸ª\n"
            
            # æ‰‹åŠ¨æ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
            manual_mapping = self.plugin_config.get('datasheet_mapping', {})
            if manual_mapping:
                status += f"ğŸ“ **æ‰‹åŠ¨æ˜ å°„**: {len(manual_mapping)} ä¸ª (å‘åå…¼å®¹)\n"
            
            return status
            
        except Exception as e:
            error_msg = f"âŒ è·å–çŠ¶æ€ä¿¡æ¯å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    async def initialize(self):
        """æ’ä»¶åˆå§‹åŒ–"""
        logger.info("ç»´æ ¼è¡¨MCPæ’ä»¶åˆå§‹åŒ–å®Œæˆ")
        # å¯åŠ¨æ—¶è‡ªåŠ¨åŒæ­¥
        await self._auto_sync_if_needed()

    async def terminate(self):
        """æ’ä»¶é”€æ¯"""
        logger.info("ç»´æ ¼è¡¨MCPæ’ä»¶å·²é”€æ¯")
