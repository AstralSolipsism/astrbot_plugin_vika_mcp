import json
import traceback
import os
import time
import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timedelta
import pytz

from astrbot.core.platform.astr_message_event import AstrMessageEvent
from astrbot.api.event import filter
from astrbot.api.star import Context, Star, register
from astrbot.api import logger, AstrBotConfig


try:
    from astral_vika import Vika
except ImportError:
    logger.error("ç»´æ ¼è¡¨MCPæ’ä»¶å¯åŠ¨å¤±è´¥ï¼šæœªæ‰¾åˆ° astral_vika åº“ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–å®‰è£…")
    Vika = None


@register("vika_mcp_plugin", "AstralSolipsism", "æ™ºèƒ½ç»´æ ¼è¡¨MCPæ’ä»¶ï¼Œæ”¯æŒè‡ªåŠ¨å‘ç°æ•°æ®è¡¨ï¼Œå¤§æ¨¡å‹è‡ªåŠ¨è°ƒç”¨å„ç§æ“ä½œåŠŸèƒ½", "0.9")
class VikaMcpPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config
        self.vika_client = None
        self.plugin_config = {}
        self.sync_lock = asyncio.Lock()
        
        # ä»é…ç½®ä¸­è¯»å–å¹¶å‘æ•°ï¼Œé»˜è®¤å€¼ä¸º5
        max_concurrent = self.plugin_config.get('max_concurrent_vika_requests', 5)
        self.vika_semaphore = asyncio.Semaphore(max_concurrent)
        
        # æ™ºèƒ½æ•°æ®è¡¨ç®¡ç†
        self.datasheet_mapping = {}  # è‡ªåŠ¨å‘ç°çš„æ•°æ®è¡¨æ˜ å°„ {name: id}
        self.datasheet_views_mapping = {} # { "datasheet_id_1": { "view_name_1": "view_id_1", ... }, ... }
        self.spaces_list = []  # ç©ºé—´ç«™åˆ—è¡¨
        self.default_space_id = None
        self.cache_file_path = os.path.join(os.getcwd(), "data", "vika_cache.json")
        self.field_meta_cache = {}  # æ–°å¢ï¼šå­—æ®µå…ƒæ•°æ®ç¼“å­˜ {datasheet_id: {field_name: field_meta}}
        self.cache_timestamp = None
        self.is_synced = False
        
        # åŠ è½½æ’ä»¶é…ç½®
        self._load_config()
        
        # åˆå§‹åŒ–ç»´æ ¼è¡¨å®¢æˆ·ç«¯
        self._init_vika_client()
        
        # åŠ è½½ç¼“å­˜
        self._load_cache()

    def _load_config(self):
        """åŠ è½½æ’ä»¶é…ç½®"""
        try:
            plugin_name = "vika_mcp_plugin"
            self.plugin_config = self.config or {}
            self.default_space_id = self.plugin_config.get('default_space_id', '')
            logger.info(f"ç»´æ ¼è¡¨MCPæ’ä»¶é…ç½®åŠ è½½å®Œæˆ: {list(self.plugin_config.keys())}")
        except Exception as e:
            logger.error(f"åŠ è½½ç»´æ ¼è¡¨MCPæ’ä»¶é…ç½®å¤±è´¥: {e}")
            self.plugin_config = {}

    def _init_vika_client(self):
        """åˆå§‹åŒ–ç»´æ ¼è¡¨å®¢æˆ·ç«¯"""
        if not Vika:
            logger.error("ç»´æ ¼è¡¨MCPæ’ä»¶åˆå§‹åŒ–å¤±è´¥ï¼švika åº“æœªå®‰è£…")
            return
            
        api_token = self.plugin_config.get('vika_api_token', '')
        vika_host = self.plugin_config.get('vika_host', 'https://api.vika.cn')
        
        if not api_token:
            logger.warning("ç»´æ ¼è¡¨ API Token æœªé…ç½®")
            return
            
        try:
            # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼Œæ”¯æŒè‡ªå®šä¹‰host
            self.vika_client = Vika(
                api_token,
                api_base=vika_host,
                status_callback=self._send_status_feedback
            )
            logger.info(f"ç»´æ ¼è¡¨å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒæœåŠ¡å™¨: {vika_host}")
        except Exception as e:
            logger.error(f"ç»´æ ¼è¡¨å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

    def _load_cache(self):
        """åŠ è½½ç¼“å­˜çš„æ•°æ®è¡¨ä¿¡æ¯"""
        try:
            if os.path.exists(self.cache_file_path):
                with open(self.cache_file_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    
                self.datasheet_mapping = cache_data.get('datasheet_mapping', {})
                self.datasheet_views_mapping = cache_data.get('datasheet_views_mapping', {})
                self.spaces_list = cache_data.get('spaces_list', [])
                self.field_meta_cache = cache_data.get('field_meta_cache', {})
                self.cache_timestamp = cache_data.get('timestamp', 0)
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                cache_duration = self.plugin_config.get('cache_duration_hours', 24)
                if time.time() - self.cache_timestamp < cache_duration * 3600:
                    self.is_synced = True
                    logger.info(f"ä»ç¼“å­˜åŠ è½½äº† {len(self.datasheet_mapping)} ä¸ªæ•°æ®è¡¨")
                else:
                    logger.info("ç¼“å­˜å·²è¿‡æœŸï¼Œéœ€è¦é‡æ–°åŒæ­¥")
            else:
                logger.info("æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œéœ€è¦åˆå§‹åŒæ­¥")
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(self.cache_file_path), exist_ok=True)
            cache_data = {
                'datasheet_mapping': self.datasheet_mapping,
                'datasheet_views_mapping': self.datasheet_views_mapping,
                'spaces_list': self.spaces_list,
                'field_meta_cache': self.field_meta_cache,
                'timestamp': time.time()
            }
            with open(self.cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logger.info("ç¼“å­˜å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _get_datasheet_id(self, datasheet_name: str) -> str:
        """æ™ºèƒ½è·å–æ•°æ®è¡¨ID"""
        # 1. é¦–å…ˆæ£€æŸ¥è‡ªåŠ¨å‘ç°çš„æ˜ å°„
        if datasheet_name in self.datasheet_mapping:
            datasheet_id = self.datasheet_mapping[datasheet_name]
            return datasheet_id
            
        # 2. æ£€æŸ¥è‡ªå®šä¹‰åˆ«å
        custom_aliases = self.plugin_config.get('custom_aliases', {})
        if datasheet_name in custom_aliases:
            real_name = custom_aliases[datasheet_name]
            if real_name in self.datasheet_mapping:
                datasheet_id = self.datasheet_mapping[real_name]
                return datasheet_id
                
        # 3. æ£€æŸ¥æ‰‹åŠ¨é…ç½®çš„æ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
        manual_mapping = self.plugin_config.get('datasheet_mapping', {})
        if datasheet_name in manual_mapping:
            datasheet_id = manual_mapping[datasheet_name]
            return datasheet_id
            
        # 4. å¦‚æœæ˜¯ç›´æ¥çš„datasheet IDæ ¼å¼ï¼Œç›´æ¥è¿”å›
        if datasheet_name.startswith('dst'):
            return datasheet_name
            
        # 5. æ¨¡ç³ŠåŒ¹é…ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
        for name, ds_id in self.datasheet_mapping.items():
            if datasheet_name.lower() in name.lower() or name.lower() in datasheet_name.lower():
                return ds_id
                
        # 6. éƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›åŸåç§°
        logger.warning(f"æœªèƒ½é€šè¿‡ä»»ä½•å·²çŸ¥æ–¹å¼è§£ææ•°æ®è¡¨åç§° '{datasheet_name}'ã€‚å°†æŒ‰åŸæ ·ä½¿ç”¨ã€‚")
        return datasheet_name

    def _get_datasheet(self, datasheet_name: str):
        """è·å–æ•°æ®è¡¨å¯¹è±¡"""
        if not self.vika_client:
            raise ValueError("ç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokenå’ŒæœåŠ¡å™¨é…ç½®")
        
        datasheet_id = self._get_datasheet_id(datasheet_name)
        if not datasheet_id:
            raise ValueError(f"æœªæ‰¾åˆ°æ•°æ®è¡¨: {datasheet_name}")
        
        return self.vika_client.datasheet(datasheet_id)
        

    async def _fetch_datasheet_views(self, datasheet_id: str) -> Dict[str, str]:
        """è·å–å•ä¸ªæ•°æ®è¡¨çš„æ‰€æœ‰è§†å›¾å¹¶ç¼“å­˜"""
        try:
            views = await self.vika_client.datasheet(datasheet_id).views.aall()
            return {view.name: view.id for view in views}
        except Exception as e:
            logger.error(f"è·å–æ•°æ®è¡¨ {datasheet_id} çš„è§†å›¾å¤±è´¥: {e}")
            return {}

    async def _fetch_datasheet_views_with_limit(self, datasheet_id: str) -> Dict[str, str]:
        """ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘åœ°è·å–å•ä¸ªæ•°æ®è¡¨çš„æ‰€æœ‰è§†å›¾"""
        async with self.vika_semaphore:
            return await self._fetch_datasheet_views(datasheet_id)

    async def _auto_sync_if_needed(self):
        """å¦‚æœéœ€è¦ä¸”é…ç½®å…è®¸ï¼Œè‡ªåŠ¨åŒæ­¥æ•°æ®è¡¨"""
        if (not self.is_synced and
            self.plugin_config.get('auto_sync_on_startup', True) and
            self.vika_client):
            async with self.sync_lock:
                if self.is_synced:  # åœ¨é”å†…è¿›è¡ŒåŒé‡æ£€æŸ¥
                    return
                # ç°åœ¨è®©è°ƒç”¨è€…å¤„ç†å¼‚å¸¸
                await self._perform_sync()

    async def _perform_sync(self):
        """æ‰§è¡Œæ•°æ®è¡¨åŒæ­¥"""
        if not self.vika_client:
            raise ValueError("ç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
        # è·å–ç©ºé—´ç«™åˆ—è¡¨
        spaces = await self.vika_client.spaces.alist()
        
        if not spaces:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•ç©ºé—´ç«™ï¼Œè¯·æ£€æŸ¥API Tokenæƒé™")
            
        self.spaces_list = [{'id': space['id'], 'name': space['name']} for space in spaces]
        
        # ç¡®å®šè¦åŒæ­¥çš„ç©ºé—´ç«™
        target_space = None
        if self.default_space_id:
            target_space = next((s for s in spaces if s['id'] == self.default_space_id), None)
        
        if not target_space:
            target_space = spaces[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç©ºé—´ç«™
            
        logger.info(f"å¼€å§‹åŒæ­¥ç©ºé—´ç«™: {target_space['name']} (ID: {target_space['id']})")
        
        # ä½¿ç”¨ v2 API ç›´æ¥æœç´¢æ‰€æœ‰æ•°æ®è¡¨èŠ‚ç‚¹ï¼Œä¼˜åŒ–åŒæ­¥æ•ˆç‡
        logger.info(f"é«˜æ•ˆåŒæ­¥ç©ºé—´ç«™ [{target_space['name']}] ä¸­çš„æ‰€æœ‰æ•°æ®è¡¨...")
        all_datasheet_nodes = await self.vika_client.space(target_space['id']).nodes.asearch(node_type='Datasheet')
        
        # å¢å¼ºæ—¥å¿—ä¸éªŒè¯
        logger.debug(f"ä»ç»´æ ¼è¡¨APIæ”¶åˆ°çš„åŸå§‹èŠ‚ç‚¹æ•°æ®: {all_datasheet_nodes}")
        if not all_datasheet_nodes:
            logger.warning(f"åœ¨ç©ºé—´ç«™ '{target_space['name']}' (ID: {target_space['id']}) ä¸­æœªå‘ç°ä»»ä½•æ•°æ®è¡¨ã€‚è¯·æ£€æŸ¥API Tokenæƒé™æˆ–ç©ºé—´ç«™å†…å®¹ã€‚")

        # éªŒè¯èŠ‚ç‚¹ç»“æ„å¹¶æ„å»ºæ˜ å°„
        new_datasheet_map = {}
        for node in all_datasheet_nodes:
            if hasattr(node, 'name') and hasattr(node, 'id'):
                new_datasheet_map[node.name] = node.id
            else:
                logger.error(f"å‘ç°ç»“æ„å¼‚å¸¸çš„èŠ‚ç‚¹å¯¹è±¡ï¼Œç¼ºå°‘'name'æˆ–'id'å±æ€§: {node}")
            
        logger.info(f"åŒæ­¥å®Œæˆã€‚å‘ç°çš„ç»´æ ¼è¡¨æ˜ å°„: {new_datasheet_map}")
        self.datasheet_mapping.update(new_datasheet_map)
        
        # è®°å½•åŒæ­¥ä¿¡æ¯
        if new_datasheet_map:
            synced_table_names = list(new_datasheet_map.keys())
            logger.info(f"æˆåŠŸåŒæ­¥ {len(synced_table_names)} ä¸ªç»´æ ¼è¡¨: {synced_table_names}")

        # å¹¶å‘è·å–æ‰€æœ‰æ•°æ®è¡¨çš„è§†å›¾ (ä½¿ç”¨é€Ÿç‡é™åˆ¶)
        logger.info("å¼€å§‹å¹¶å‘è·å–æ‰€æœ‰æ•°æ®è¡¨çš„è§†å›¾ï¼ˆå¸¦é€Ÿç‡é™åˆ¶ï¼‰...")
        tasks = [self._fetch_datasheet_views_with_limit(ds_id) for ds_id in self.datasheet_mapping.values()]
        view_results = await asyncio.gather(*tasks)
        
        # æ›´æ–°è§†å›¾æ˜ å°„ç¼“å­˜
        datasheet_ids = list(self.datasheet_mapping.values())
        for i, views_map in enumerate(view_results):
            datasheet_id = datasheet_ids[i]
            if views_map:
                self.datasheet_views_mapping[datasheet_id] = views_map
        logger.info(f"è§†å›¾ä¿¡æ¯åŒæ­¥å®Œæˆï¼Œå…±å¤„ç† {len(self.datasheet_views_mapping)} ä¸ªæ•°æ®è¡¨çš„è§†å›¾ã€‚")
            
        self.is_synced = True
        self.cache_timestamp = time.time()
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_cache()
        
        return len(new_datasheet_map)

    def _format_records_to_json(self, records: List[Any]) -> str:
        """å°†è®°å½•åˆ—è¡¨æ ¼å¼åŒ–ä¸ºJSONå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿AIç†è§£ã€‚"""
        if not records:
            return json.dumps([])

        record_list = []
        for record in records:
            # å‡è®¾ record å¯¹è±¡æœ‰ .id å’Œ .fields å±æ€§
            record_dict = {
                "record_id": getattr(record, 'id', ''),
                "fields": getattr(record, 'fields', {})
            }
            record_list.append(record_dict)
        
        return json.dumps(record_list, ensure_ascii=False, indent=2)

    def _format_records_for_display(self, records: List[Dict[str, Any]], limit: int = None) -> str:
        """æ ¼å¼åŒ–è®°å½•ä¸ºå¯è¯»çš„æ–‡æœ¬æ ¼å¼"""
        if not records:
            return "æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®°å½•"

        max_display = limit or self.plugin_config.get('max_records_display', 20)
        display_records = records[:max_display]
        
        # æ„å»ºè¡¨æ ¼æ˜¾ç¤º
        result = f"æ‰¾åˆ° {len(records)} æ¡è®°å½•"
        if len(records) > max_display:
            result += f"ï¼ˆæ˜¾ç¤ºå‰ {max_display} æ¡ï¼‰"
        result += ":\\n\\n"
        
        if display_records:
            # è·å–æ‰€æœ‰å­—æ®µå
            all_fields = set()
            for record in display_records:
                record_fields = getattr(record, 'fields', None)
                if record_fields:
                    all_fields.update(record_fields.keys())
            
            field_list = list(all_fields)
            
            # è¡¨å¤´
            result += "| " + " | ".join(field_list) + " |\\n"
            result += "|" + "---|" * len(field_list) + "\\n"
            
            # æ•°æ®è¡Œ
            for record in display_records:
                fields = getattr(record, 'fields', {})
                row_data = []
                for field in field_list:
                    value = fields.get(field, '')
                    # å¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œé•¿æ–‡æœ¬
                    if isinstance(value, str):
                        value = value.replace('|', '\\|').replace('\\n', ' ')
                        if len(value) > 30:
                            value = value[:30] + "..."
                    elif value is None:
                        value = ""
                    row_data.append(str(value))
                result += "| " + " | ".join(row_data) + " |\\n"
            
        return result

    async def _get_field_meta(self, datasheet) -> Dict[str, Any]:
        """è·å–å¹¶ç¼“å­˜æ•°æ®è¡¨çš„å­—æ®µå…ƒæ•°æ®"""
        if datasheet.dst_id in self.field_meta_cache:
            return self.field_meta_cache[datasheet.dst_id]
        
        fields = await datasheet.fields.aall()
        meta = {field.name: field for field in fields}
        self.field_meta_cache[datasheet.dst_id] = meta
        self._save_cache() # ä¿å­˜åˆ°ç¼“å­˜
        return meta

    async def _resolve_field_values(self, datasheet, record_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ™ºèƒ½è§£æå’Œè½¬æ¢å­—æ®µå€¼ã€‚
        å°†ç”¨æˆ·å‹å¥½çš„è¾“å…¥ï¼ˆå¦‚å§“åã€é€‰é¡¹æ–‡æœ¬ã€å…³è”è®°å½•æ ‡é¢˜ï¼‰è½¬æ¢ä¸ºVika APIæ‰€éœ€çš„IDæ ¼å¼ã€‚
        """
        field_meta = await self._get_field_meta(datasheet)
        resolved_fields = {}

        for field_name, user_value in record_data.items():
            if field_name not in field_meta:
                # å¦‚æœå­—æ®µä¸å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„å€¼
                resolved_fields[field_name] = user_value
                continue

            field_info = field_meta[field_name]
            field_type = field_info.type

            # --- æˆå‘˜å­—æ®µè§£æ ---
            if field_type in ["Member", "CreatedBy", "LastModifiedBy"] and isinstance(user_value, str):
                space_id = datasheet.space_id
                members = await self.vika_client.space(space_id).nodes.asearch(node_type='Member')
                member_map = {member.name: member.id for member in members}
                if user_value in member_map:
                    resolved_fields[field_name] = [member_map[user_value]]
                else:
                    logger.warning(f"åœ¨ç©ºé—´ç«™ä¸­æœªæ‰¾åˆ°æˆå‘˜ '{user_value}'ï¼Œå°†æŒ‰åŸæ ·å‘é€")
                    resolved_fields[field_name] = user_value
                continue

            # --- å•é€‰/å¤šé€‰å­—æ®µè§£æ ---
            elif field_type in ["SingleSelect", "MultiSelect"]:
                options_map = {opt['name']: opt['id'] for opt in field_info.property['options']}
                if isinstance(user_value, list): # å¤šé€‰
                    resolved_ids = [options_map[val] for val in user_value if val in options_map]
                    resolved_fields[field_name] = resolved_ids
                elif isinstance(user_value, str) and user_value in options_map: # å•é€‰
                    resolved_fields[field_name] = options_map[user_value]
                else:
                    logger.warning(f"åœ¨å­—æ®µ '{field_name}' çš„é€‰é¡¹ä¸­æœªæ‰¾åˆ° '{user_value}'")
                    resolved_fields[field_name] = user_value
                continue

            # --- å…³è”è®°å½•å­—æ®µè§£æ ---
            elif field_type in ["OneWayLink", "TwoWayLink"] and isinstance(user_value, (str, list)):
                foreign_dst_id = field_info.property.get('foreignDatasheetId')
                if not foreign_dst_id:
                    resolved_fields[field_name] = user_value
                    continue
                
                foreign_datasheet = self.vika_client.datasheet(foreign_dst_id)
                
                # ç¡®ä¿ä¼ å…¥çš„æ˜¯åˆ—è¡¨
                search_titles = user_value if isinstance(user_value, list) else [user_value]
                
                # ä½¿ç”¨ in æŸ¥è¯¢æ‰¹é‡è·å–è®°å½•
                # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾å…³è”è®°å½•çš„æ ‡é¢˜æ˜¯ä¸»å­—æ®µï¼Œä¸”å¯ä»¥ç›´æ¥æŸ¥è¯¢
                # Vika APIç›®å‰ä¸ç›´æ¥æ”¯æŒæŒ‰æ ‡é¢˜æ‰¹é‡æŸ¥è¯¢ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå¾ªç¯æŸ¥è¯¢ï¼Œå®é™…åº”ç”¨ä¸­å¯ä¼˜åŒ–
                record_ids = []
                for title in search_titles:
                    try:
                        # å‡è®¾ä¸»å­—æ®µåä¸º'æ ‡é¢˜'æˆ–'åç§°'ï¼Œè¿™éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                        # æ›´ç¨³å¥çš„åšæ³•æ˜¯è·å–å…³è”è¡¨çš„ç¬¬ä¸€ä¸ªæ–‡æœ¬å­—æ®µä½œä¸ºæŸ¥è¯¢å­—æ®µ
                        records = await foreign_datasheet.records.filter(f"{{ä¸»å­—æ®µ}}='{title}'").aall()
                        if records:
                            record_ids.append(records[0].id)
                        else:
                            logger.warning(f"åœ¨å…³è”è¡¨ '{foreign_dst_id}' ä¸­æœªæ‰¾åˆ°æ ‡é¢˜ä¸º '{title}' çš„è®°å½•")
                    except Exception as e:
                        logger.error(f"æŸ¥è¯¢å…³è”è®°å½•å¤±è´¥: {e}")

                if record_ids:
                    resolved_fields[field_name] = record_ids
                else:
                    resolved_fields[field_name] = user_value
                continue

            # --- æ—¥æœŸæ—¶é—´è§£æ (å¯æ‰©å±•) ---
            elif field_type == "DateTime" and isinstance(user_value, str):
                try:
                    # å°è¯•å¤šç§æ ¼å¼
                    dt = datetime.fromisoformat(user_value.replace('Z', '+00:00'))
                    resolved_fields[field_name] = int(dt.timestamp() * 1000)
                except ValueError:
                    logger.warning(f"æ—¥æœŸæ ¼å¼ '{user_value}' è§£æå¤±è´¥ï¼Œå°†æŒ‰åŸæ ·å‘é€")
                    resolved_fields[field_name] = user_value
                continue

            # --- å…¶ä»–å­—æ®µç›´æ¥èµ‹å€¼ ---
            else:
                resolved_fields[field_name] = user_value
        
        return resolved_fields

    async def _build_formula_from_filter(self, datasheet, filter_by: Dict[str, Any]) -> str:
        """
        ä» filter_by å­—å…¸æ„å»º Vika æŸ¥è¯¢å…¬å¼ã€‚
        """
        if not filter_by:
            return ""

        field_meta = await self._get_field_meta(datasheet)
        formula_parts = []

        for field_name, value in filter_by.items():
            if field_name not in field_meta:
                # å¦‚æœå­—æ®µä¸å­˜åœ¨ï¼Œå°è¯•è¿›è¡Œç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
                formula_parts.append(f"{{{field_name}}}='{value}'")
                continue

            field_info = field_meta[field_name]
            field_type = field_info.type

            # --- å•é€‰/å¤šé€‰å­—æ®µè§£æ ---
            if field_type == "SingleSelect":
                options_map = {opt['name']: opt['id'] for opt in field_info.property['options']}
                if isinstance(value, str) and value in options_map:
                    formula_parts.append(f"{{{field_name}}}='{options_map[value]}'")
                else:
                    logger.warning(f"åœ¨å­—æ®µ '{field_name}' çš„é€‰é¡¹ä¸­æœªæ‰¾åˆ° '{value}'ï¼Œå°†å¿½ç•¥æ­¤ç­›é€‰æ¡ä»¶")
            
            elif field_type == "MultiSelect":
                options_map = {opt['name']: opt['id'] for opt in field_info.property['options']}
                values_to_check = value if isinstance(value, list) else [value]
                ids = [options_map.get(v) for v in values_to_check if v in options_map]
                if ids:
                    # å¯¹äºå¤šé€‰ï¼Œæ£€æŸ¥æ¯ä¸ªIDæ˜¯å¦å­˜åœ¨
                    or_parts = [f"FIND('{i}', ARRAYJOIN({{{field_name}}}))" for i in ids]
                    formula_parts.append(f"OR({','.join(or_parts)})")
                else:
                    logger.warning(f"åœ¨å­—æ®µ '{field_name}' çš„é€‰é¡¹ä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆå€¼ï¼Œå°†å¿½ç•¥æ­¤ç­›é€‰æ¡ä»¶")

            # --- æˆå‘˜å­—æ®µè§£æ ---
            elif field_type == "Member":
                space_id = datasheet.space_id
                members = await self.vika_client.space(space_id).nodes.asearch(node_type='Member')
                member_map = {member.name: member.id for member in members}
                if value in member_map:
                    formula_parts.append(f"{{{field_name}}}='{member_map[value]}'")
                else:
                    logger.warning(f"åœ¨ç©ºé—´ç«™ä¸­æœªæ‰¾åˆ°æˆå‘˜ '{value}'ï¼Œå°†å¿½ç•¥æ­¤ç­›é€‰æ¡ä»¶")

            # --- å…³è”è®°å½•å­—æ®µè§£æ ---
            elif field_type in ["OneWayLink", "TwoWayLink"]:
                 # æ³¨æ„ï¼šæŒ‰å…³è”è®°å½•çš„æ–‡æœ¬æ ‡é¢˜è¿›è¡Œè¿‡æ»¤é€šå¸¸å¾ˆå¤æ‚ä¸”æ•ˆç‡ä½ä¸‹ã€‚
                 # Vika API æœ¬èº«ä¸æ”¯æŒç›´æ¥è¿™æ ·åšã€‚è¿™é‡Œæˆ‘ä»¬å‡è®¾ç”¨æˆ·æä¾›äº†è®°å½•IDã€‚
                 # å¦‚æœç”¨æˆ·æä¾›çš„æ˜¯æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæœç´¢IDï¼Œè¿™ä¼šå¢åŠ å¤æ‚æ€§ã€‚
                 # ç›®å‰ï¼Œæˆ‘ä»¬åªæ”¯æŒæŒ‰è®°å½•IDè¿‡æ»¤ã€‚
                if isinstance(value, str) and value.startswith('rec'):
                    formula_parts.append(f"{{{field_name}}}='{value}'")
                else:
                    logger.warning(f"å…³è”å­—æ®µ '{field_name}' çš„ç­›é€‰ç›®å‰åªæ”¯æŒè®°å½•ID (rec...)ï¼Œå·²å¿½ç•¥å€¼ '{value}'")
            
            # --- å…¶ä»–å­—æ®µç›´æ¥è¿›è¡Œç­‰å€¼æ¯”è¾ƒ ---
            else:
                if isinstance(value, str):
                    formula_parts.append(f"{{{field_name}}}='{value}'")
                elif isinstance(value, (int, float)):
                    formula_parts.append(f"{{{field_name}}}={value}")
                else:
                    logger.warning(f"ä¸æ”¯æŒå¯¹å­—æ®µ '{field_name}' çš„å€¼ '{value}' (ç±»å‹: {type(value)}) è¿›è¡Œç­›é€‰")

        if not formula_parts:
            return ""
        
        return f"AND({','.join(formula_parts)})"

    @filter.llm_tool(name="sync_vika_datasheets")
    async def sync_vika_datasheets(self, event: AstrMessageEvent) -> str:
        """åŒæ­¥å¹¶ç¼“å­˜ç»´æ ¼ç©ºé—´ç«™ä¸­çš„æ‰€æœ‰æ•°æ®è¡¨ï¼Œè®©æ‚¨å¯ä»¥é€šè¿‡åç§°ç›´æ¥æ“ä½œå®ƒä»¬ã€‚
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        try:
            discovered_count = await self._perform_sync()
            
            result = f"âœ… åŒæ­¥å®Œæˆï¼å…±å‘ç° {discovered_count} ä¸ªæ•°æ®è¡¨ã€‚\n\n"
            result += "ğŸ“‹ **å·²å‘ç°çš„æ•°æ®è¡¨**ï¼š\n"
            
            for name, ds_id in list(self.datasheet_mapping.items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                result += f"â€¢ {name} (`{ds_id}`)\n"
                
            if len(self.datasheet_mapping) > 10:
                result += f"â€¢ ... è¿˜æœ‰ {len(self.datasheet_mapping) - 10} ä¸ªæ•°æ®è¡¨\n"
                
            result += "\nğŸ’¡ ç°åœ¨æ‚¨å¯ä»¥ç›´æ¥é€šè¿‡æ•°æ®è¡¨åç§°æ¥æ“ä½œå®ƒä»¬äº†ï¼"
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ åŒæ­¥ç»´æ ¼è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="list_vika_spaces")
    async def list_vika_spaces(self, event: AstrMessageEvent) -> str:
        """åˆ—å‡ºæ‚¨åœ¨ç»´æ ¼è¡¨å¹³å°ä¸­åˆ›å»ºæˆ–æœ‰æƒè®¿é—®çš„æ‰€æœ‰ç©ºé—´ç«™ï¼ˆå³è¡¨æ ¼çš„ç»„ç»‡å®¹å™¨ï¼‰ã€‚
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        try:
            spaces = await self.vika_client.spaces.alist()
            
            if not spaces:
                return "ğŸ“­ æœªæ‰¾åˆ°ä»»ä½•ç©ºé—´ç«™ï¼Œè¯·æ£€æŸ¥æ‚¨çš„API Tokenæƒé™"
                
            result = f"ğŸ¢ **æ‚¨çš„ç»´æ ¼è¡¨ç©ºé—´ç«™** (å…± {len(spaces)} ä¸ª)ï¼š\n\n"
            
            for space in spaces:
                is_default = " ğŸ”¸ *é»˜è®¤*" if space['id'] == self.default_space_id else ""
                result += f"â€¢ **{space['name']}**{is_default}\n"
                result += f"  ID: `{space['id']}`\n\n"
                
            if not self.default_space_id and len(spaces) > 1:
                result += "ğŸ’¡ **æç¤º**: å¦‚æœæ‚¨æœ‰å¤šä¸ªç©ºé—´ç«™ï¼Œå»ºè®®åœ¨é…ç½®ä¸­è®¾ç½® `default_space_id` ä»¥æŒ‡å®šé»˜è®¤æ“ä½œçš„ç©ºé—´ç«™ã€‚"
                
            return result
            
        except Exception as e:
            error_msg = f"âŒ è·å–ç©ºé—´ç«™åˆ—è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="list_vika_datasheets")
    async def list_vika_datasheets(self, event: AstrMessageEvent, space_id: str = None, filter_keyword: str = None) -> str:
        """åˆ—å‡ºç»´æ ¼è¡¨ä¸­çš„æ‰€æœ‰å…·ä½“æ•°æ®è¡¨æ ¼ï¼ˆDatasheetï¼‰ï¼Œæ”¯æŒæŒ‡å®šç©ºé—´ç«™å’Œå…³é”®è¯è¿‡æ»¤ã€‚

        Args:
            space_id(string): å¯é€‰ï¼ŒæŒ‡å®šè¦åˆ—å‡ºæ•°æ®è¡¨çš„ç©ºé—´ç«™IDã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨é»˜è®¤ç©ºé—´ç«™æˆ–å·²åŒæ­¥çš„æ•°æ®è¡¨ã€‚
            filter_keyword(string): å¯é€‰ï¼Œè¿‡æ»¤å…³é”®è¯ï¼Œåªæ˜¾ç¤ºåŒ…å«è¯¥å…³é”®è¯çš„æ•°æ®è¡¨ã€‚
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"

        try:
            datasheets_to_list = {}
            space_id_to_query = space_id
            
            # å¦‚æœç”¨æˆ·æ²¡æœ‰æä¾› space_idï¼Œåˆ™æ£€æŸ¥å¹¶ä½¿ç”¨é»˜è®¤é…ç½®
            if not space_id_to_query:
                space_id_to_query = self.default_space_id

            if space_id_to_query and isinstance(space_id_to_query, str):
                try:
                    # å§‹ç»ˆä½¿ç”¨é«˜æ•ˆçš„æœç´¢API
                    logger.info(f"ä½¿ç”¨é«˜æ•ˆæœç´¢APIåœ¨ç©ºé—´ç«™ [{space_id_to_query}] ä¸­æŸ¥æ‰¾æ‰€æœ‰æ•°æ®è¡¨...")
                    all_datasheet_nodes = await self.vika_client.space(space_id_to_query).nodes.asearch(node_type='Datasheet')
                    datasheets_to_list = {node.name: node.id for node in all_datasheet_nodes}
                except Exception as e:
                    logger.error(f"æ— æ³•è·å–ç©ºé—´ç«™ '{space_id_to_query}' ä¸­çš„æ•°æ®è¡¨: {e}\n{traceback.format_exc()}")
                    return f"âŒ æ— æ³•è·å–ç©ºé—´ç«™ '{space_id_to_query}' ä¸­çš„æ•°æ®è¡¨ï¼Œè¯·æ£€æŸ¥ç©ºé—´ç«™IDå’Œæƒé™: {str(e)}"
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šspace_idï¼Œåˆ™ä½¿ç”¨å·²åŒæ­¥çš„æ•°æ®è¡¨
                await self._auto_sync_if_needed()
                if not self.is_synced:
                    return "âš ï¸ æ•°æ®è¡¨åˆ—è¡¨å°šæœªåŒæ­¥ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®è¡¨åŒæ­¥åŠŸèƒ½ã€‚"
                datasheets_to_list = self.datasheet_mapping

            if not datasheets_to_list:
                return "ğŸ“­ æœªå‘ç°ä»»ä½•æ•°æ®è¡¨ï¼Œè¯·æ£€æŸ¥ç©ºé—´ç«™ä¸­æ˜¯å¦æœ‰æ•°æ®è¡¨ï¼Œæˆ–é‡æ–°åŒæ­¥ã€‚"

            # åº”ç”¨è¿‡æ»¤
            filtered_tables = {}
            if filter_keyword:
                for name, ds_id in datasheets_to_list.items():
                    if filter_keyword.lower() in name.lower():
                        filtered_tables[name] = ds_id
            else:
                filtered_tables = datasheets_to_list

            if not filtered_tables:
                return f"ğŸ” æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ '{filter_keyword}' çš„æ•°æ®è¡¨ã€‚"

            result = f"ğŸ“Š **æ•°æ®è¡¨åˆ—è¡¨**"
            if space_id_to_query:
                result += f" (æ¥è‡ªç©ºé—´ç«™: `{space_id_to_query}`)"
            if filter_keyword:
                result += f" (åŒ…å« '{filter_keyword}')"
            result += f" (å…± {len(filtered_tables)} ä¸ª)ï¼š\n\n"

            for name, ds_id in filtered_tables.items():
                result += f"â€¢ **{name}**\n"
                result += f"  ID: `{ds_id}`\n\n"

            # æ˜¾ç¤ºè‡ªå®šä¹‰åˆ«åæç¤º
            custom_aliases = self.plugin_config.get('custom_aliases', {})
            if custom_aliases:
                result += "ğŸ·ï¸ **è‡ªå®šä¹‰åˆ«å**ï¼š\n"
                for alias, real_name in custom_aliases.items():
                    if real_name in self.datasheet_mapping: # ä»…æ˜¾ç¤ºå·²åŒæ­¥çš„åˆ«å
                        result += f"â€¢ `{alias}` â†’ {real_name}\n"
                result += "\n"

            result += "ğŸ’¡ **æç¤º**: æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æ•°æ®è¡¨åç§°è¿›è¡Œæ“ä½œï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«ã€‚"
            
            return result

        except Exception as e:
            error_msg = f"âŒ æŸ¥è¯¢è¡¨æ ¼å¤±è´¥ï¼š{str(e)}ã€‚è¯·æ£€æŸ¥æ‚¨çš„API Tokenæƒé™æˆ–ç©ºé—´ç«™IDæ˜¯å¦æ­£ç¡®ï¼Œå¹¶æŸ¥çœ‹åå°æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚"
            logger.error(f"è·å–æ•°æ®è¡¨åˆ—è¡¨å¤±è´¥: {e}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="create_vika_datasheet")
    async def create_vika_datasheet(self, event: AstrMessageEvent, datasheet_name: str, fields: List[Dict[str, Any]]) -> str:
        """åœ¨ç»´æ ¼è¡¨ç©ºé—´ç«™ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®è¡¨ï¼Œæ”¯æŒå®šä¹‰å¤æ‚çš„å­—æ®µå±æ€§ã€‚

        Args:
            datasheet_name(string): è¦åˆ›å»ºçš„æ•°æ®è¡¨çš„åç§°ã€‚
            fields(array): å­—æ®µåˆ—è¡¨ã€‚æ¯ä¸ªå­—æ®µæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå¿…é¡»åŒ…å« "name" å’Œ "type"ã€‚
                         å¯¹äºå¤æ‚å­—æ®µï¼Œå¯ä»¥é¢å¤–æä¾› "property" å­—å…¸ã€‚
                         - å•é€‰/å¤šé€‰: `{"name": "çŠ¶æ€", "type": "SingleSelect", "property": {"options": [{"name": "å¾…å¤„ç†"}, {"name": "è¿›è¡Œä¸­"}]}}`
                         - æ™ºèƒ½å…¬å¼: `{"name": "æ€»ä»·", "type": "Formula", "property": {"expression": "{æ•°é‡} * {å•ä»·}"}}`
                         - å…³è”ä»–è¡¨: `{"name": "å…³è”é¡¹ç›®", "type": "OneWayLink", "property": {"foreignDatasheetId": "dst_xxx"}}`
                         - æŒ‰é’®: `{"name": "è§¦å‘æ“ä½œ", "type": "Button", "property": {"label": "ç‚¹å‡»è¿è¡Œ", "style": "primary"}}`
        """
        if not self.vika_client:
            return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"
            
        try:
            # ç¡®å®šç›®æ ‡ç©ºé—´ç«™
            spaces = await self.vika_client.spaces.alist()
            
            if not spaces:
                return "âŒ æœªæ‰¾åˆ°ä»»ä½•ç©ºé—´ç«™ï¼Œè¯·æ£€æŸ¥API Tokenæƒé™"
                
            target_space = None
            if self.default_space_id:
                target_space = next((s for s in spaces if s['id'] == self.default_space_id), None)
            
            if not target_space:
                target_space = spaces[0]
                
            # åˆ›å»ºæ•°æ®è¡¨
            create_params = {
                'name': datasheet_name,
                'fields': fields,
                'folderId': target_space['id']
            }
            
            new_datasheet = await self.vika_client.space(target_space['id']).datasheets.acreate(**create_params)
            
            # æ›´æ–°æœ¬åœ°ç¼“å­˜
            self.datasheet_mapping[datasheet_name] = new_datasheet.id
            self._save_cache()
            
            result = f"âœ… æ•°æ®è¡¨åˆ›å»ºæˆåŠŸï¼\n\n"
            result += f"ğŸ“Š **æ•°æ®è¡¨åç§°**: {datasheet_name}\n"
            result += f"ğŸ†” **æ•°æ®è¡¨ID**: `{new_datasheet.id}`\n"
            result += f"ğŸ¢ **æ‰€åœ¨ç©ºé—´ç«™**: {target_space['name']}\n"
            
            if fields:
                result += f"ğŸ“‹ **å­—æ®µæ•°é‡**: {len(fields)} ä¸ª\n"
                
            result += "\nğŸ’¡ æ•°æ®è¡¨å·²æ·»åŠ åˆ°ç¼“å­˜ï¼Œæ‚¨å¯ä»¥ç›´æ¥é€šè¿‡åç§°æ“ä½œå®ƒã€‚"
            
            return result
            
        except Exception as e:
            error_msg = f"âŒ åˆ›å»ºæ•°æ®è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    async def _check_sync_and_suggest(self, datasheet_name: str) -> Optional[str]:
        """æ£€æŸ¥æ•°æ®è¡¨æ˜¯å¦åŒæ­¥ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™æä¾›å»ºè®®"""
        if not self.vika_client:
            return "âŒ ç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"

        # ç¡®ä¿åœ¨æ£€æŸ¥å‰å·²å°è¯•åŒæ­¥
        await self._auto_sync_if_needed()
            
        # å°è¯•æ™ºèƒ½è·å–æ•°æ®è¡¨ID
        datasheet_id = self._get_datasheet_id(datasheet_name)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®è¡¨ï¼Œæä¾›å»ºè®®
        if datasheet_id == datasheet_name and not datasheet_name.startswith('dst'):
            if not self.is_synced:
                msg = f"âš ï¸ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}'ï¼Œä¸”æ•°æ®è¡¨åˆ—è¡¨è‡ªåŠ¨åŒæ­¥å¤±è´¥ã€‚è¯·æ£€æŸ¥é…ç½®æˆ–æ‰‹åŠ¨åŒæ­¥ã€‚"
                return msg
            else:
                # æä¾›ç›¸ä¼¼çš„æ•°æ®è¡¨å»ºè®®
                similar_tables = []
                for table_name in self.datasheet_mapping.keys():
                    if any(word in table_name.lower() for word in datasheet_name.lower().split()):
                        similar_tables.append(table_name)
                
                if similar_tables:
                    suggestion = f"âŒ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}'ã€‚æ‚¨æ˜¯å¦æƒ³è¦æ“ä½œï¼š\n"
                    for table in similar_tables[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªå»ºè®®
                        suggestion += f"â€¢ {table}\n"
                    return suggestion
                else:
                    available_tables = list(self.datasheet_mapping.keys())[:5]
                    msg = (f"âŒ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}'ã€‚\n"
                           f"å¯ç”¨çš„æ•°æ®è¡¨ï¼š{', '.join(available_tables)}")
                    return msg
        
        return None  # æ‰¾åˆ°äº†æ•°æ®è¡¨ï¼Œæ— éœ€æç¤º

    @filter.llm_tool(name="get_vika_records")
    async def get_vika_records(
        self,
        event: AstrMessageEvent,
        datasheet_name: str,
        filter_by: Optional[Dict[str, Any]] = None,
        formula: str = None,
        view_id: str = None,
        sort: str = None,
        fields: str = None,
        max_records: str = None,
        page_size: str = None,
        page_num: str = None,
        field_key: str = None,
        cell_format: str = None
    ) -> str:
        """
        æŸ¥è¯¢ç»´æ ¼è¡¨è®°å½•ã€‚è¿™æ˜¯è·å–ã€è¿‡æ»¤å’Œæœç´¢æ•°æ®çš„å”¯ä¸€ä¸”åŠŸèƒ½æœ€å¼ºå¤§çš„æ–¹æ³•ã€‚

        æ”¯æŒä¸‰ç§è¿‡æ»¤æ–¹å¼ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰:
        1. `formula`: ä½¿ç”¨ç»´æ ¼è¡¨åŸç”Ÿå…¬å¼è¿›è¡Œå¤æ‚æŸ¥è¯¢ï¼ŒåŠŸèƒ½æœ€å¼ºã€‚
        2. `filter_by`: ä½¿ç”¨ç®€å•çš„ "å­—æ®µ: å€¼" å­—å…¸è¿›è¡Œç­‰å€¼æŸ¥è¯¢ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†å¤æ‚å­—æ®µã€‚
        3. å¦‚éœ€è¿›è¡Œå…³é”®è¯æœç´¢ï¼Œè¯·ä½¿ç”¨ `formula` å‚æ•°ï¼Œä¾‹å¦‚ `formula="FIND('å…³é”®è¯', {è¦æœç´¢çš„å­—æ®µå})"`ã€‚

        Args:
            datasheet_name(string): è¦æŸ¥è¯¢çš„æ•°æ®è¡¨çš„å‡†ç¡®åç§°æˆ–åˆ«åã€‚
            filter_by(object): å¯é€‰ï¼Œä¸€ä¸ªç®€å•çš„é”®å€¼å¯¹å­—å…¸ï¼Œç”¨äºè¿‡æ»¤è®°å½•ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†å¤æ‚å­—æ®µã€‚
                               - `{"çŠ¶æ€": "å¤„ç†ä¸­"}` (ä¼šè‡ªåŠ¨è§£æä¸ºå•é€‰å­—æ®µçš„ID)
                               - `{"è´Ÿè´£äºº": "å¼ ä¸‰"}` (ä¼šè‡ªåŠ¨è§£æä¸ºæˆå‘˜å­—æ®µçš„ID)
                               - `{"æ ‡ç­¾": ["é‡è¦", "ç´§æ€¥"]}` (ä¼šè‡ªåŠ¨è§£æä¸ºå¤šé€‰å­—æ®µçš„ID)
                               - `{"å…³è”é¡¹ç›®": "recAbc123"}` (å…³è”å­—æ®µç›®å‰ä»…æ”¯æŒæŒ‰è®°å½•IDæŸ¥è¯¢)
            formula(string): å¯é€‰ï¼Œä¸€ä¸ªç»´æ ¼è¡¨æŸ¥è¯¢å…¬å¼ã€‚åŠŸèƒ½æœ€å¼ºå¤§ï¼Œæ¨èä½¿ç”¨ã€‚å¦‚æœåŒæ—¶æä¾›äº† filter_byï¼Œæ­¤é¡¹ä¼˜å…ˆã€‚
                         - **ç²¾ç¡®åŒ¹é…**: `"{æ ‡é¢˜}='å¼ ä¸‰çš„æŠ¥å‘Š'"`
                         - **æ¨¡ç³Šæœç´¢**: `"FIND('æŠ¥å‘Š', {æ ‡é¢˜})"`
                         - **ç»„åˆæ¡ä»¶**: `"AND({çŠ¶æ€}='å·²å®Œæˆ', {åˆ†æ•°}>=60)"`
            view_id(string): å¯é€‰ï¼Œè¦æŸ¥è¯¢çš„è§†å›¾IDæˆ–è§†å›¾åç§°ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™æŸ¥è¯¢æ‰€æœ‰è®°å½•ã€‚
            sort(string): å¯é€‰ï¼Œæ’åºå­—æ®µã€‚å¤šä¸ªå­—æ®µç”¨é€—å·åˆ†éš”ï¼Œé™åºåœ¨å­—æ®µåå‰åŠ '-'ã€‚ç¤ºä¾‹: 'å­—æ®µA,-å­—æ®µB'ã€‚
            fields(string): å¯é€‰ï¼ŒæŒ‡å®šè¿”å›çš„å­—æ®µã€‚å¤šä¸ªå­—æ®µç”¨é€—å·åˆ†éš”ã€‚ç¤ºä¾‹: 'å­—æ®µA,å­—æ®µB'ã€‚
            max_records(string): å¯é€‰ï¼ŒæŒ‡å®šæœ¬æ¬¡è¯·æ±‚è¿”å›çš„æœ€å¤§è®°å½•æ•°ã€‚
            page_size(string): å¯é€‰ï¼ŒæŒ‡å®šæ¯é¡µè¿”å›çš„è®°å½•æ•°ï¼Œä¸ page_num é…åˆä½¿ç”¨ã€‚
            page_num(string): å¯é€‰ï¼ŒæŒ‡å®šåˆ†é¡µçš„é¡µç ï¼Œä¸ page_size é…åˆä½¿ç”¨ã€‚
            field_key(string): å¯é€‰ï¼ŒæŒ‡å®šå­—æ®µçš„è¿”å›é”®ï¼Œ'name' (å­—æ®µå) æˆ– 'id' (å­—æ®µID)ã€‚é»˜è®¤ä¸º 'name'ã€‚
            cell_format(string): å¯é€‰ï¼ŒæŒ‡å®šå•å…ƒæ ¼å€¼çš„è¿”å›æ ¼å¼ï¼Œ'json' æˆ– 'string'ã€‚é»˜è®¤ä¸º 'json'ã€‚
        """
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œ get_vika_records, datasheet_name='{datasheet_name}', filter_by={filter_by}")
            if not self.vika_client:
                return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"

            # --- å‚æ•°ç±»å‹è½¬æ¢å’ŒéªŒè¯ ---
            parsed_max_records = None
            if max_records:
                try:
                    parsed_max_records = int(max_records)
                except (ValueError, TypeError):
                    return f"âŒ å‚æ•° 'max_records' çš„å€¼ '{max_records}' å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ•´æ•°ã€‚"

            parsed_page_size = None
            if page_size:
                try:
                    parsed_page_size = int(page_size)
                except (ValueError, TypeError):
                    return f"âŒ å‚æ•° 'page_size' çš„å€¼ '{page_size}' å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ•´æ•°ã€‚"

            parsed_page_num = None
            if page_num:
                try:
                    parsed_page_num = int(page_num)
                except (ValueError, TypeError):
                    return f"âŒ å‚æ•° 'page_num' çš„å€¼ '{page_num}' å¿…é¡»æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ•´æ•°ã€‚"
            # --- å‚æ•°å¤„ç†ç»“æŸ ---

            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
            
            datasheet = self._get_datasheet(datasheet_name)
            logger.info(f"æˆåŠŸå®šä½æ•°æ®è¡¨: {datasheet.dst_id}")

            # --- æŸ¥è¯¢é€»è¾‘ ---
            final_formula = formula
            if not final_formula and filter_by:
                logger.info(f"æ­£åœ¨ä» filter_by æ„å»ºæŸ¥è¯¢å…¬å¼: {filter_by}")
                final_formula = await self._build_formula_from_filter(datasheet, filter_by)
                logger.info(f"æ„å»ºçš„æŸ¥è¯¢å…¬å¼ä¸º: {final_formula}")
            
            if formula and filter_by:
                logger.warning(f"åŒæ—¶æä¾›äº† 'formula' å’Œ 'filter_by'ï¼Œå°†ä¼˜å…ˆä½¿ç”¨ 'formula': {formula}")

            # åˆå§‹åŒ–æŸ¥è¯¢é›†
            qs = datasheet.records.all()

            # è§£æè§†å›¾ID
            final_view_id = view_id
            if view_id and not view_id.startswith('viw'):
                datasheet_id = datasheet.dst_id
                if datasheet_id in self.datasheet_views_mapping and view_id in self.datasheet_views_mapping[datasheet_id]:
                    final_view_id = self.datasheet_views_mapping[datasheet_id][view_id]
                    logger.info(f"å·²å°†è§†å›¾åç§° '{view_id}' è§£æä¸º ID '{final_view_id}'")
                else:
                    logger.warning(f"æœªåœ¨ç¼“å­˜ä¸­æ‰¾åˆ°åä¸º '{view_id}' çš„è§†å›¾ï¼Œå°†æŒ‰åŸæ ·ä½¿ç”¨ã€‚")

            # åº”ç”¨å„ç§æŸ¥è¯¢å‚æ•°
            if final_formula:
                qs = qs.filter(final_formula)
                logger.info(f"ä½¿ç”¨å…¬å¼è¿›è¡ŒæŸ¥è¯¢: {final_formula}")
            if final_view_id:
                qs = qs.view(final_view_id)
                logger.info(f"æŒ‰è§†å›¾è¿›è¡ŒæŸ¥è¯¢: {final_view_id}")
            if fields:
                qs = qs.fields(*[f.strip() for f in fields.split(',')])
            if sort:
                qs = qs.sort(*[s.strip() for s in sort.split(',')])
            if parsed_max_records is not None:
                qs = qs.limit(parsed_max_records)
            if parsed_page_size is not None:
                qs = qs.page_size(parsed_page_size)
            if parsed_page_num is not None:
                qs = qs.page_num(parsed_page_num)
            if field_key:
                qs = qs.field_key(field_key)
            if cell_format:
                qs = qs.cell_format(cell_format)

            # æ‰§è¡ŒæŸ¥è¯¢
            records = []
            try:
                logger.info("æ­£åœ¨å°è¯•è¿æ¥ç»´æ ¼è¡¨å¹¶æŸ¥è¯¢æ•°æ®...")
                records = await qs.aall()
                logger.info(f"æˆåŠŸä»ç»´æ ¼è¡¨APIè·å–åˆ° {len(records)} æ¡è®°å½•ã€‚")
            except Exception as e:
                logger.error(f"æŸ¥è¯¢ç»´æ ¼è¡¨APIæ—¶å‘ç”Ÿå¼‚å¸¸: {e}\n{traceback.format_exc()}")
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç”±å¤–å±‚æ•è·

            # æ ¼å¼åŒ–ä¸ºJSONå¹¶è¿”å›
            formatted_content = self._format_records_to_json(records)
            
            logger.info(f"æˆåŠŸå¤„ç†æ•°æ®è¡¨ '{datasheet_name}' çš„ {len(records)} æ¡è®°å½•ã€‚")
            
            return formatted_content

        except Exception as e:
            error_msg = f"âŒ æŸ¥è¯¢ç»´æ ¼è¡¨ '{datasheet_name}' å†…å®¹å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="add_vika_record")
    async def add_vika_record(
        self,
        datasheet_name: str,
        record_data: Dict[str, Any]
    ) -> str:
        """å‘æŒ‡å®šçš„ç»´æ ¼è¡¨ä¸­æ·»åŠ æ–°è®°å½•ï¼Œæ”¯æŒæ™ºèƒ½è§£æå¤æ‚å­—æ®µã€‚

        Args:
            datasheet_name(string): è¦æ·»åŠ è®°å½•çš„æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰ã€‚
            record_data(object): è®°å½•æ•°æ®ï¼Œä¸€ä¸ªåŒ…å«å­—æ®µåå’Œå¯¹åº”å€¼çš„å­—å…¸ã€‚
                                 - **æ™®é€šæ–‡æœ¬/æ•°å­—/æ—¥æœŸ**: `{"æ ‡é¢˜": "æ–°ä»»åŠ¡", "æ•°é‡": 10, "æˆªæ­¢æ—¥æœŸ": "2024-12-31"}`
                                 - **æˆå‘˜å­—æ®µ**: `{"è´Ÿè´£äºº": "å¼ ä¸‰"}` (ä¼šè‡ªåŠ¨åŒ¹é…å§“åå¹¶è½¬æ¢ä¸ºç”¨æˆ·ID)
                                 - **å•é€‰å­—æ®µ**: `{"çŠ¶æ€": "å¾…å¤„ç†"}` (ä¼šè‡ªåŠ¨åŒ¹é…é€‰é¡¹æ–‡æœ¬)
                                 - **å¤šé€‰å­—æ®µ**: `{"æ ‡ç­¾": ["é‡è¦", "ç´§æ€¥"]}` (ä¼šè‡ªåŠ¨åŒ¹é…å¤šä¸ªé€‰é¡¹æ–‡æœ¬)
                                 - **å…³è”è®°å½•**: `{"å…³è”é¡¹ç›®": "é¡¹ç›®Açš„æ ‡é¢˜"}` æˆ– `{"å…³è”é¡¹ç›®": ["é¡¹ç›®A", "é¡¹ç›®B"]}` (ä¼šè‡ªåŠ¨æŸ¥æ‰¾å¹¶å…³è”è®°å½•ID)
                                 - **é™„ä»¶**: `{"é™„ä»¶å­—æ®µ": [{"token": "attxxxx"}]}` (éœ€è¦å…ˆä¸Šä¼ æ–‡ä»¶è·å–token)
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            if not record_data:
                return "âŒ é”™è¯¯ï¼šè®°å½•æ•°æ®ä¸ºç©ºã€‚"
            
            # æ™ºèƒ½è§£æå­—æ®µæ•°æ®
            resolved_fields = await self._resolve_field_values(datasheet, record_data)
            
            # ç›´æ¥æ‰§è¡Œå¼‚æ­¥æ“ä½œ
            result = await datasheet.records.acreate(resolved_fields)
            
            logger.info(f"æˆåŠŸå‘ç»´æ ¼è¡¨ [{datasheet_name}] æ·»åŠ äº† 1 æ¡æ–°è®°å½•: {result.id}")
            return f"âœ… æˆåŠŸæ·»åŠ è®°å½•åˆ°æ•°æ®è¡¨ '{datasheet_name}'ï¼Œè®°å½•ID: {result.id}"
            
        except Exception as e:
            error_msg = f"âŒ æ·»åŠ ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg


    @filter.llm_tool(name="update_vika_record")
    async def update_vika_record(self, event: AstrMessageEvent, datasheet_name: str, record_id: str, record_data: Dict[str, Any]) -> str:
        """æ›´æ–°æŒ‡å®šç»´æ ¼è¡¨ä¸­çš„è®°å½•ï¼Œæ”¯æŒæ™ºèƒ½è§£æå¤æ‚å­—æ®µã€‚

        Args:
            datasheet_name(string): è®°å½•æ‰€åœ¨çš„æ•°æ®è¡¨çš„åç§°ã€‚
            record_id(string): è¦æ›´æ–°çš„è®°å½•çš„ IDã€‚
            record_data(object): ä¸€ä¸ªåŒ…å«è¦æ›´æ–°çš„å­—æ®µå’Œæ–°å€¼çš„å­—å…¸ã€‚è¾“å…¥æ ¼å¼ä¸'add_vika_record'å®Œå…¨ç›¸åŒã€‚
                                 ä¾‹å¦‚: `{"çŠ¶æ€": "å·²å®Œæˆ", "è´Ÿè´£äºº": "æå››"}`
                                 - **æ›´æ–°é™„ä»¶**: `{"é™„ä»¶å­—æ®µ": [{"token": "attnewtoken"}]}`
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            if not record_data:
                return "âŒ é”™è¯¯ï¼šæ›´æ–°æ•°æ®ä¸ºç©º"
            
            # æ™ºèƒ½è§£æå­—æ®µæ•°æ®
            resolved_fields = await self._resolve_field_values(datasheet, record_data)
            
            # ç›´æ¥æ‰§è¡Œå¼‚æ­¥æ“ä½œ
            await datasheet.records.aupdate([{'recordId': record_id, 'fields': resolved_fields}])
            
            logger.info(f"æˆåŠŸåœ¨ç»´æ ¼è¡¨ [{datasheet_name}] ä¸­æ›´æ–°äº† 1 æ¡è®°å½•: {record_id}")
            return f"âœ… æˆåŠŸæ›´æ–°æ•°æ®è¡¨ '{datasheet_name}' ä¸­çš„è®°å½• {record_id}"
            
        except Exception as e:
            error_msg = f"âŒ æ›´æ–°ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="delete_vika_record")
    async def delete_vika_record(self, event: AstrMessageEvent, datasheet_name: str, record_id: str) -> str:
        """åˆ é™¤æŒ‡å®šç»´æ ¼è¡¨ä¸­çš„è®°å½•ã€‚

        Args:
            datasheet_name(string): è®°å½•æ‰€åœ¨çš„æ•°æ®è¡¨çš„åç§°ã€‚
            record_id(string): è¦åˆ é™¤çš„è®°å½•çš„ IDã€‚
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # ç›´æ¥æ‰§è¡Œå¼‚æ­¥æ“ä½œ
            await datasheet.records.adelete(record_id)
            
            return f"âœ… æˆåŠŸåˆ é™¤æ•°æ®è¡¨ '{datasheet_name}' ä¸­çš„è®°å½• {record_id}"
            
        except Exception as e:
            error_msg = f"âŒ åˆ é™¤ç»´æ ¼è¡¨è®°å½•å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="get_vika_fields")
    async def get_vika_fields(
        self,
        datasheet_name: str
    ) -> str:
        """è·å–æŒ‡å®šç»´æ ¼è¡¨çš„å­—æ®µä¿¡æ¯ã€‚

        Args:
            datasheet_name(string): æ•°æ®è¡¨åç§°æˆ–åˆ«åï¼ˆå¿…éœ€ï¼‰
        """
        try:
            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
                
            datasheet = self._get_datasheet(datasheet_name)
            
            # è·å–å­—æ®µä¿¡æ¯
            fields = await datasheet.fields.aall()
            
            if not fields:
                return f"ğŸ“‹ æ•°æ®è¡¨ '{datasheet_name}' æ²¡æœ‰å­—æ®µä¿¡æ¯"
            
            result = f"ğŸ“‹ **æ•°æ®è¡¨ '{datasheet_name}' çš„å­—æ®µä¿¡æ¯** (å…± {len(fields)} ä¸ªå­—æ®µ):\n\n"
            for field in fields:
                result += f"â€¢ **{field.name}** (ç±»å‹: {field.type})"
                if hasattr(field, 'description') and field.description:
                    result += f" - {field.description}"
                result += "\n"
            
            result += "\nğŸ’¡ æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›å­—æ®µåæ¥æ·»åŠ æˆ–æ›´æ–°è®°å½•ã€‚"
            return result
            
        except Exception as e:
            error_msg = f"âŒ è·å–ç»´æ ¼è¡¨å­—æ®µä¿¡æ¯å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="get_vika_status")
    async def get_vika_status(self) -> str:
        """æ£€æŸ¥ç»´æ ¼è¡¨æ’ä»¶çš„è¿æ¥çŠ¶æ€ã€é…ç½®ä¿¡æ¯å’Œæ•°æ®è¡¨åŒæ­¥çŠ¶æ€ã€‚
        """
        try:
            # ç¡®ä¿çŠ¶æ€æ£€æŸ¥å‰å·²å°è¯•åŒæ­¥
            await self._auto_sync_if_needed()
            
            status = "ğŸ”§ **ç»´æ ¼è¡¨MCPæ’ä»¶çŠ¶æ€**\n\n"
            
            # æ£€æŸ¥å®¢æˆ·ç«¯çŠ¶æ€
            if self.vika_client:
                status += "âœ… **å®¢æˆ·ç«¯è¿æ¥**: æ­£å¸¸\n"
            else:
                status += "âŒ **å®¢æˆ·ç«¯è¿æ¥**: æœªå»ºç«‹\n"
            
            # æ£€æŸ¥é…ç½®
            api_token = self.plugin_config.get('vika_api_token', '')
            vika_host = self.plugin_config.get('vika_host', 'https://api.vika.cn')
            
            if api_token:
                status += f"ğŸ”‘ **API Token**: å·²é…ç½® ({api_token[:8]}...)\n"
            else:
                status += "ğŸ”‘ **API Token**: âŒ æœªé…ç½®\n"
            
            status += f"ğŸŒ **æœåŠ¡å™¨åœ°å€**: {vika_host}\n"
            
            # ç©ºé—´ç«™ä¿¡æ¯
            if self.spaces_list:
                status += f"ğŸ¢ **ç©ºé—´ç«™**: å…± {len(self.spaces_list)} ä¸ª\n"
                if self.default_space_id:
                    default_space = next((s for s in self.spaces_list if s['id'] == self.default_space_id), None)
                    if default_space:
                        status += f"ğŸ”¸ **é»˜è®¤ç©ºé—´ç«™**: {default_space['name']}\n"
            else:
                status += "ğŸ¢ **ç©ºé—´ç«™**: æœªåŒæ­¥\n"
            
            # æ•°æ®è¡¨åŒæ­¥çŠ¶æ€
            if self.is_synced:
                cache_age = int((time.time() - self.cache_timestamp) / 3600) if self.cache_timestamp else 0
                status += f"ğŸ“Š **æ•°æ®è¡¨åŒæ­¥**: âœ… å·²åŒæ­¥ ({len(self.datasheet_mapping)} ä¸ªè¡¨)\n"
                status += f"ğŸ• **ç¼“å­˜æ—¶é—´**: {cache_age} å°æ—¶å‰\n"
                
                # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®è¡¨
                if self.datasheet_mapping:
                    status += "ğŸ“‹ **éƒ¨åˆ†æ•°æ®è¡¨**:\n"
                    for i, (name, _) in enumerate(list(self.datasheet_mapping.items())[:5]):
                        status += f"   â€¢ {name}\n"
                    if len(self.datasheet_mapping) > 5:
                        status += f"   â€¢ ... è¿˜æœ‰ {len(self.datasheet_mapping) - 5} ä¸ª\n"
            else:
                status += "ğŸ“Š **æ•°æ®è¡¨åŒæ­¥**: âŒ æœªåŒæ­¥\n"
                status += "ğŸ’¡ **å»ºè®®**: è¿è¡ŒåŒæ­¥åŠŸèƒ½æ¥å‘ç°æ‚¨çš„æ•°æ®è¡¨\n"
            
            # é…ç½®ç‰¹æ€§
            auto_sync = self.plugin_config.get('auto_sync_on_startup', True)
            cache_duration = self.plugin_config.get('cache_duration_hours', 24)
            max_records = self.plugin_config.get('max_records_display', 20)
            
            status += f"\nâš™ï¸ **é…ç½®ä¿¡æ¯**:\n"
            status += f"ğŸ”„ **å¯åŠ¨åŒæ­¥**: {'âœ… å·²å¯ç”¨' if auto_sync else 'âŒ å·²ç¦ç”¨'}\n"
            status += f"ğŸ’¾ **ç¼“å­˜æ—¶é•¿**: {cache_duration} å°æ—¶\n"
            status += f"ğŸ“Š **æœ€å¤§æ˜¾ç¤º**: {max_records} æ¡è®°å½•\n"
            
            # è‡ªå®šä¹‰åˆ«å
            custom_aliases = self.plugin_config.get('custom_aliases', {})
            if custom_aliases:
                status += f"ğŸ·ï¸ **è‡ªå®šä¹‰åˆ«å**: {len(custom_aliases)} ä¸ª\n"
            
            # æ‰‹åŠ¨æ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
            manual_mapping = self.plugin_config.get('datasheet_mapping', {})
            if manual_mapping:
                status += f"ğŸ“ **æ‰‹åŠ¨æ˜ å°„**: {len(manual_mapping)} ä¸ª (å‘åå…¼å®¹)\n"
            
            return status
            
        except Exception as e:
            error_msg = f"âŒ è·å–çŠ¶æ€ä¿¡æ¯å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="list_vika_views")
    def list_vika_views(self, event: AstrMessageEvent, datasheet_name: str) -> str:
        """åˆ—å‡ºæŒ‡å®šæ•°æ®è¡¨çš„æ‰€æœ‰å¯ç”¨è§†å›¾ã€‚

        Args:
            datasheet_name(string): è¦æŸ¥è¯¢è§†å›¾çš„æ•°æ®è¡¨çš„åç§°ã€‚
        """
        try:
            datasheet_id = self._get_datasheet_id(datasheet_name)
            if not datasheet_id or datasheet_id not in self.datasheet_views_mapping:
                return f"âŒ æœªæ‰¾åˆ°æ•°æ®è¡¨ '{datasheet_name}' çš„è§†å›¾ä¿¡æ¯ã€‚è¯·å…ˆåŒæ­¥æˆ–æ£€æŸ¥åç§°ã€‚"

            views = self.datasheet_views_mapping.get(datasheet_id, {})
            if not views:
                return f"ğŸ“‹ æ•°æ®è¡¨ '{datasheet_name}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§†å›¾ã€‚"

            result = f"ğŸ“‹ **æ•°æ®è¡¨ '{datasheet_name}' çš„è§†å›¾åˆ—è¡¨** (å…± {len(views)} ä¸ª):\n\n"
            for view_name, view_id in views.items():
                result += f"â€¢ **{view_name}**\n"
                result += f"  ID: `{view_id}`\n"
            
            result += "\nğŸ’¡ æ‚¨å¯ä»¥åœ¨æŸ¥è¯¢æ—¶ä½¿ç”¨ `view_id` æ¥è·å–ç‰¹å®šè§†å›¾çš„æ•°æ®ã€‚"
            return result

        except Exception as e:
            error_msg = f"âŒ è·å–è§†å›¾åˆ—è¡¨å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg
    @filter.llm_tool(name="get_current_time")
    async def get_current_time(
        self,
        event: AstrMessageEvent,
        timezone_str: str = 'UTC',
        format_str: str = None
    ) -> str:
        """
        è·å–å½“å‰æ—¶é—´ï¼Œæ”¯æŒæŒ‡å®šæ—¶åŒºå’Œæ ¼å¼ã€‚

        Args:
            timezone_str(string): å¯é€‰ï¼Œç”¨äºæŒ‡å®šæ—¶åŒºï¼Œä¾‹å¦‚ 'Asia/Shanghai' æˆ– 'UTC'ã€‚é»˜è®¤ä¸º 'UTC'ã€‚
            format_str(string): å¯é€‰ï¼Œç”¨äºæŒ‡å®šè¿”å›æ—¶é—´çš„æ ¼å¼ï¼Œéµå¾ª strftime æ ‡å‡†ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™è¿”å› ISO 8601 æ ¼å¼çš„å­—ç¬¦ä¸²ã€‚
        """
        try:
            # è·å–å½“å‰UTCæ—¶é—´
            now_utc = datetime.utcnow().replace(tzinfo=pytz.utc)

            # è®¾ç½®ç›®æ ‡æ—¶åŒº
            target_tz = pytz.timezone(timezone_str)
            now_local = now_utc.astimezone(target_tz)

            # æ ¼å¼åŒ–è¾“å‡º
            if format_str:
                return now_local.strftime(format_str)
            else:
                return now_local.isoformat()
        except pytz.UnknownTimeZoneError:
            return f"âŒ é”™è¯¯ï¼šæœªçŸ¥çš„æ—¶åŒº '{timezone_str}'"
        except Exception as e:
            error_msg = f"âŒ è·å–å½“å‰æ—¶é—´å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    @filter.llm_tool(name="upload_vika_attachment")
    async def upload_vika_attachment(
        self,
        event: AstrMessageEvent, 
        datasheet_name: str, 
        file_path: str,
        record_id: str = None,
        field_name: str = None
    ) -> str:
        """å°†æœ¬åœ°æ–‡ä»¶ä½œä¸ºé™„ä»¶ä¸Šä¼ åˆ°ç»´æ ¼è¡¨ï¼Œå¹¶å¯é€‰æ‹©ç›´æ¥å…³è”åˆ°æŒ‡å®šè®°å½•çš„ç‰¹å®šå­—æ®µã€‚

        Args:
            datasheet_name(string): é™„ä»¶è¦ä¸Šä¼ åˆ°çš„æ•°æ®è¡¨çš„åç§°ã€‚
            file_path(string): è¦ä¸Šä¼ çš„æœ¬åœ°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
            record_id(string): å¯é€‰ï¼Œè¦å°†é™„ä»¶æ·»åŠ åˆ°çš„è®°å½•çš„IDã€‚
            field_name(string): å¯é€‰ï¼Œè¦æ·»åŠ é™„ä»¶çš„å­—æ®µåç§°ã€‚å¦‚æœæä¾›äº† record_idï¼Œæ­¤é¡¹ä¸ºå¿…éœ€ã€‚
        """
        try:
            if not self.vika_client:
                return "âŒ é”™è¯¯ï¼šç»´æ ¼è¡¨å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API Tokené…ç½®"

            if not os.path.exists(file_path):
                return f"âŒ é”™è¯¯ï¼šæ–‡ä»¶æœªæ‰¾åˆ° '{file_path}'"

            if record_id and not field_name:
                return "âŒ é”™è¯¯ï¼šå½“æä¾› record_id æ—¶ï¼Œå¿…é¡»åŒæ—¶æä¾› field_nameã€‚"

            # æ£€æŸ¥åŒæ­¥çŠ¶æ€å¹¶æä¾›å»ºè®®
            sync_check = await self._check_sync_and_suggest(datasheet_name)
            if sync_check:
                return sync_check
            
            datasheet = self._get_datasheet(datasheet_name)
            logger.info(f"å‡†å¤‡å‘æ•°æ®è¡¨ '{datasheet_name}' (ID: {datasheet.dst_id}) ä¸Šä¼ æ–‡ä»¶: {file_path}")

            # æ­¥éª¤ 1: æ‰§è¡Œä¸Šä¼ 
            attachment = await datasheet.attachments.aupload(file_path)
            logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {attachment.name} (Token: {attachment.token})")

            # æ­¥éª¤ 2: å¦‚æœæä¾›äº†è®°å½•IDå’Œå­—æ®µåï¼Œåˆ™ç›´æ¥æ›´æ–°è®°å½•
            if record_id and field_name:
                logger.info(f"å‡†å¤‡å°†é™„ä»¶å…³è”åˆ°è®°å½• '{record_id}' çš„å­—æ®µ '{field_name}'")
                update_data = {
                    'recordId': record_id,
                    'fields': {
                        field_name: [
                            {'token': attachment.token}
                        ]
                    }
                }
                await datasheet.records.aupdate([update_data])
                logger.info(f"æˆåŠŸå°†é™„ä»¶å…³è”åˆ°è®°å½• {record_id}")
                return (
                    f"âœ… æ–‡ä»¶ä¸Šä¼ å¹¶æˆåŠŸå…³è”ï¼\n\n"
                    f"ğŸ“„ **æ–‡ä»¶å**: {attachment.name}\n"
                    f"ğŸ“¦ **å¤§å°**: {attachment.size / 1024:.2f} KB\n"
                    f"ğŸ”— **å·²å…³è”åˆ°**: æ•°æ®è¡¨ '{datasheet_name}' -> è®°å½• '{record_id}' -> å­—æ®µ '{field_name}'"
                )

            # å¦‚æœæ²¡æœ‰æä¾›è®°å½•IDï¼Œåˆ™åªè¿”å›é™„ä»¶ä¿¡æ¯
            result = (
                f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼\n\n"
                f"ğŸ“„ **æ–‡ä»¶å**: {attachment.name}\n"
                f"ğŸ“¦ **å¤§å°**: {attachment.size / 1024:.2f} KB\n"
                f"ğŸ”— **URL**: {attachment.url}\n"
                f"ğŸ”‘ **é™„ä»¶Token**: `{attachment.token}`\n\n"
                f"ğŸ’¡ **æç¤º**: æ‚¨å¯ä»¥åœ¨'æ·»åŠ è®°å½•'æˆ–'æ›´æ–°è®°å½•'æ—¶ï¼Œåœ¨ç›¸åº”çš„é™„ä»¶å­—æ®µä¸­ä½¿ç”¨æ­¤é™„ä»¶Tokenã€‚"
            )
            return result

        except Exception as e:
            error_msg = f"âŒ ä¸Šä¼ é™„ä»¶åˆ° '{datasheet_name}' å¤±è´¥: {str(e)}"
            logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return error_msg

    async def _send_status_feedback(self, message: str):
        """å‘é€çŠ¶æ€åé¦ˆç»™ç”¨æˆ·"""
        try:
            if self.context and hasattr(self.context, 'event') and self.context.event:
                await self.context.event.reply(f"ã€ç»´æ ¼è¡¨ã€‘{message}")
            else:
                logger.debug(f"æ— æ³•å‘é€çŠ¶æ€åé¦ˆï¼ˆeventä¸å¯ç”¨ï¼‰: {message}")
        except Exception as e:
            logger.error(f"å‘é€çŠ¶æ€åé¦ˆå¤±è´¥: {e}")

    async def _delayed_auto_sync(self):
        """å»¶è¿Ÿæ‰§è¡Œè‡ªåŠ¨åŒæ­¥ï¼Œä»¥ç¡®ä¿eventå¯¹è±¡å·²å‡†å¤‡å¥½"""
        await asyncio.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿ
        try:
            await self._auto_sync_if_needed()
        except Exception as e:
            logger.error(f"å»¶è¿Ÿè‡ªåŠ¨åŒæ­¥å¤±è´¥: {e}")
            await self._send_status_feedback(f"è‡ªåŠ¨åŒæ­¥å¤±è´¥: {e}")

    async def initialize(self):
        """æ’ä»¶åˆå§‹åŒ–"""
        logger.info("ç»´æ ¼è¡¨MCPæ’ä»¶åˆå§‹åŒ–å®Œæˆ")
        # ä½¿ç”¨å»¶è¿Ÿä»»åŠ¡æ¥ç¡®ä¿eventå¯¹è±¡å¯ç”¨
        asyncio.create_task(self._delayed_auto_sync())

    async def terminate(self):
        """æ’ä»¶é”€æ¯"""
        logger.info("ç»´æ ¼è¡¨MCPæ’ä»¶å·²é”€æ¯")
